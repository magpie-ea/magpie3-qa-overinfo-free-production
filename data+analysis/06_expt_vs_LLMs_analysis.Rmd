---
title: "QA free typing analysis"
author: "Polina Tsvilodub"
date: '2023-01-02'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(tidyboot)
library(aida)
library(brms)
library(tidybayes)
# remotes::install_github("coolbutuseless/ggpattern")
library(ggpattern)
```

```{r, include=FALSE}
# these options help Stan run faster
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values = project_colors)
} 
```

## Intro

The following script compares the results from the [free production human experiment](https://github.com/magpie-ea/magpie3-qa-overinfo-free-production/blob/main/data%2Banalysis/05_main_free_typing_cogsci_analysis.md) run on the final 30 items for CogSci to samples and scores retrieved from Language Models (LMs) and neural models fine-tuned for question answering on various datasets. For each model, the top 5 predictions were retrieved. For LMs, the results are compared when using the one-shot example also used for GPT-3, vs results without any additional context prompting. Example model queries looked like this:

* extractive QA models: [CLS] Do you have raspberry cake? [SEP] You are a server in a café. Today the café has raspberry pie, chocolate cookies, and cheese pizza. A customer asks:[SEP]
* LM: You are a server in a café. Today the café has raspberry pie, chocolate cookies, and cheese pizza. A customer asks: Q: Do you have raspberry cake? A:
* LM with one shot example: You are hosting a barbecue party. You are standing behind the barbecue. You have the following goods to offer: pork sausages, vegan burgers, grilled potatoes and beef burgers. You reason about what that person most likely wanted to have. That they asked for grilled zucchini suggests that they might want vegetarian food. From the items you have pork sausages and beef burgers are least likely to satisfy the persons desires. Vegan burgers and grilled potatoes come much closer. Grilled potatoes are most similar to grilled zucchini. You reply: I'm sorry, I don't have any grilled zucchini. But I do have some grilled potatoes. Now consider a different situation. Someone asks: Do you have grilled zucchini? You are a server in a café. Today the café has raspberry pie, chocolate cookies, and cheese pizza. A customer asks: Q: Do you have raspberry cake? A:

The samples from the neural models were hand-categorized into the same categories as the human data. That is, they were categorized according to the following criteria. Note that responses consisting of incomplete / ungrammatical / repetitive sentences (especially in case of GPT-2 based or one-shot models, the predictions sometimes repeated parts of the context)  were also classified as long as they contained mentions of critical alternatives. When the alternative was mentioned incompletely (e.g., 'kickboxing' instead of 'kickboxing class'), it was only taken into account in the classification of it was judged to be a phrase hat could be naturally produced by human participants (mostly, kickboxing item). 

* 'competitor': responses mentioning the anticipated competitor only. Responses which started with yes but then only mentioned the competitor were also considered competitor category responses. 
* 'sameCategory': responses offering both same category alternatives or offering the option which we did not consider the direct competitor. Responses which started with yes but then mentioned relevant alternatives were also considered same category responses. 
* 'otherCategory': responses offering the alternative from the different category. Responses which started with yes but then mentioned relevant alternatives were also considered other category responses.
* 'fullList': responses where all alternatives were listed (also across several sentences). Responses which started with yes or repeated parts of the context / question but then mentioned all alternatives were also considered fullList responses.
* 'taciturn': responses not offering any alternative options or further alternative solutions. Responses repeating the question and saying 'no' or saying 'no' followed by some (generated) explanation or repetition were also considered taciturn responses.
* 'other': where a same category + other category response are mixed, uncertain answers, unclassifiable responses, responses offering further steps towards solcing the problem, responses using basic level categories (e.g., "dogs" instead of offering specific alternatives). Also nonsense responses, contradictory responses, responses mentioning parts of the one-shot context, completely ungrammatical responses, responses including insufficient formulations of some alternative were classified as 'other'.
* 'yes': plain 'yes' responses, responses mentioning that the target item was present, responses mentioning the presense of the target otem even if it was followed by correctly mentioning alternative items (e.g., as additional options).
* The additional 'none' category was introduced due to extractive QA models which sometime predict an empty span consisting of a special token only. These were marked as being silent, i.e., 'none' (since such an option did not exist for human participants).

First, the script provides some descriptive information about the neural model samples and probs, before presenting visual comparisons to human data followed by some exploratory stats.

## Load processed experimental results and neural model data, display descriptions

Read in by-vignette response category proportions.

```{r, include=FALSE, warning=FALSE, message=FALSE}
answerOrder <- c( 'competitor', 'sameCategory', 'otherCategory', 'fullList', 'taciturn', 'other')
#df <- read_csv("../../raw_data/results_103_QA-overinfo-freeTyping-cogsci_PT_full.csv")
#df %>% select(-prolific_pid, -prolific_session_id, -prolific_study_id) %>%  write_csv("data/results_QA-overinfo-freeTyping-cogsci_full_anonymized.csv")

df_human <- read_csv("data/results_QA-overinfo-freeTyping-cogsci_full_anonymized_categorized_byItem_summary.csv") %>% 
  mutate(model_name = "human") %>%
  rename(., c('prop' = 'responseCategory_proportion'))
df_human_global <- read_csv("data/results_QA-overinfo-freeTyping-cogsci_full_anonymized_categorized_global_summary.csv") %>% 
  mutate(model_name = "human") %>% 
  rename(., c('prop' = 'answerType_proportion')) %>%
  select(-answerType_count)

head(df_human)
head(df_human_global)
```

Read in categorized sample data from (generative) LMs and extractive QA models.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# iterate over all classified sample csvs
#nm_samples_categorized <-  data.frame()
#all_sample_files <- list.files("../code/results/categorized_responses", pattern="*.csv", full.names = TRUE)
nm_samples_categorized <- read_csv("../data_paper_neural/QA_neural_models_categorized_samples_E1.csv")

for (f in all_sample_files) {
  df <- read_csv(f)
  if (grepl("fewShot", f)) {
    df <- df %>%
      mutate(is_few_shot = TRUE)
  } else {
    df <- df %>%
      mutate(is_few_shot = FALSE)
  }
  # rename QA cols
  if ("predicted_spans" %in% colnames(df)) {
    df <- df %>%
      rename(., c('predictions' = 'predicted_spans'))
  } else if ("predictions_cleaned" %in% colnames(df)) {
    df <- df %>% select(-predictions) %>%
      rename(., c('predictions' = 'predictions_cleaned'))      
  } 
  if ('prediction_probs' %in% colnames(df)) {
    df <- df %>% 
      rename(., c('probs' = 'prediction_probs')) 
  }
  df <- df %>% select(model_name, itemName, predictions, category, probs, is_few_shot)
  nm_samples_categorized <- rbind(nm_samples_categorized, df)
}
head(nm_samples_categorized)
```

Load all files with probabilities of different kinds of (human) responses under pretrained LMs. All probabilities were computed *without* one-shot examples in the context (except for GPT3).
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#lm_scores <- data.frame()
#all_score_files <- list.files("../code/results/lm_probs", pattern="*.csv", full.names = TRUE)
lm_scores <- read_csv("../data_paper_neural/e1_SA_model_probs.csv")

for (f in all_score_files) {
  print(f)
  df <- read_csv(f)
  # rename GPT cols
  if ("predictions_cleaned" %in% colnames(df)) {
    df <- df %>% select(-predictions) %>%
      rename(., c('predictions' = 'predictions_cleaned'))      
  } 
  lm_scores <- rbind(lm_scores, df)
}
head(lm_scores)
```

Look at the distributions of response types in the sampled responses, globally and by-model. 'None' and 'yes' responses are excluded from global, by-model, by-item and by-model-by-item proportion summaries for further analyses.

```{r, echo=FALSE}
nm_samples_categorized <- nm_samples_categorized %>%
  mutate(
    dataset = case_when(
      model_name == 'aware-ai/bart-squadv2' ~ 'SQuADv2',
      model_name == 'danyaljj/gpt2_question_answering_squad2' ~ 'SQuADv2',
      model_name == 'deepset/bert-base-cased-squad2' ~ 'SQuADv2',
      model_name == 'deepset/deberta-v3-base-squad2' ~ 'SQuADv2',
      model_name == 'deepset/electra-base-squad2' ~ 'SQuADv2',
      model_name == 'deepset/roberta-base-squad2' ~ 'SQuADv2',
      model_name == 'deepset/tinyroberta-squad2' ~ 'SQuADv2',
      model_name == 'yjernite/bart_eli5' ~ 'eli5',
      model_name == 'bert-large-uncased-whole-word-masking-finetuned-squad' ~ 'SQuADv1',
      model_name == 'distilbert-base-cased-distilled-squad' ~ 'SQuADv1',
      model_name == 'distilbert-base-uncased-distilled-squad' ~ 'SQuADv1',
      model_name == 'bigscience/T0_3B' ~ 'MC QA, EQA, CBQA, D2T, sentiment, summary, topic, paraphrase',
      model_name == 'gpt2' ~ 'WebText',
      model_name == 'MaRiOrOsSi/t5-base-finetuned-question-answering' ~ 'DuoRC',
      model_name == 'valhalla/t5-base-qa-qg-hl' ~ 'SQuADv1',
      model_name == 'chatGPT' ~ 'CC, WebText, Books, Wikipedia, human data',
      model_name == 'gpt3' ~ 'CC, WebText, Books, Wikipedia',
      TRUE ~ 'unknown'
    ),
    model_type = case_when(
      model_name == 'aware-ai/bart-squadv2' ~ 'extractive',
      model_name == 'danyaljj/gpt2_question_answering_squad2' ~ 'generative',
      model_name == 'deepset/bert-base-cased-squad2' ~ 'extractive',
      model_name == 'deepset/deberta-v3-base-squad2' ~ 'extractive',
      model_name == 'deepset/electra-base-squad2' ~ 'extractive',
      model_name == 'deepset/roberta-base-squad2' ~ 'extractive',
      model_name == 'deepset/tinyroberta-squad2' ~ 'extractive',
      model_name == 'yjernite/bart_eli5' ~ 'generative',
      model_name == 'bert-large-uncased-whole-word-masking-finetuned-squad' ~ 'extractive',
      model_name == 'distilbert-base-cased-distilled-squad' ~ 'extractive',
      model_name == 'distilbert-base-uncased-distilled-squad' ~ 'extractive',
      model_name == 'bigscience/T0_3B' ~ 'generative',
      model_name == 'gpt2' ~ 'generative',
      model_name == 'MaRiOrOsSi/t5-base-finetuned-question-answering' ~ 'generative',
      model_name == 'valhalla/t5-base-qa-qg-hl' ~ 'generative',
      model_name == 'gpt3' ~ 'generative',
      model_name == 'gpt3-davinci-003' ~ 'generative',
      model_name == 'chatGPT' ~ 'generative',
      TRUE ~ 'unknown'
    )
  )
```

```{r, echo=FALSE}
cat("Models that were used: ", nm_samples_categorized %>% pull(model_name) %>% unique())
cat("Number of models that were used: ", nm_samples_categorized %>% pull(model_name) %>% unique() %>% length())

cat("Overall response category counts:")
nm_samples_categorized %>% count(category)

cat("By model response category counts: ")
nm_samples_categorized %>% count(model_name, category) 

cat("By dataset response category proportions: ")
nm_samples_categorized %>% group_by(dataset, category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  arrange(desc(prop))

cat("Compare response category proportions of models trained on SQuADv1 vs SQuADv2: ")
nm_samples_categorized %>% group_by(dataset, category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  select(-n) %>%
  filter((dataset == "SQuADv1") | (dataset == "SQuADv2")) %>%
  pivot_wider(names_from = "dataset", values_from = "prop")

cat("Compare response category proportions of cased vs uncased DistilBERT models: ")
nm_samples_categorized %>% group_by(model_name, category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  select(-n) %>%
  filter((model_name == "distilbert-base-cased-distilled-squad") | (model_name == "distilbert-base-uncased-distilled-squad")) %>%
  pivot_wider(names_from = "model_name", values_from = "prop")

df_samples_byModel_byContext_summary <- nm_samples_categorized %>% group_by(model_name, is_few_shot, category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n))
cat("Proportion of response categories by model: ")
df_samples_byModel_byContext_summary

cat("Proportion of 'yes' response categories by model (sorted from worst model to best): ")
df_samples_byModel_byContext_summary %>% filter(category == 'yes') %>% arrange(desc(prop))

cat("Proportion of 'none' response categories by QA model (sorted from worst model to best): ")
df_samples_byModel_byContext_summary %>% filter(category == 'none') %>% arrange(desc(prop))

df_samples_byModel_byContext_summary <- nm_samples_categorized %>% 
  filter((category != 'yes') & (category != 'none')) %>%
  group_by(model_name, is_few_shot, category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  mutate(answerType = factor(category, levels=answerOrder))

df_samples_byModel_summary <- nm_samples_categorized %>% 
  filter((category != 'yes') & (category != 'none')) %>%
  group_by(model_name, category) %>%
  summarize(n = n(), mean_probs = mean(probs)) %>%
  mutate(prop = n / sum(n)) %>%
  mutate(answerType = factor(category, levels=answerOrder))

df_samples_byModelType_summary <- nm_samples_categorized %>% 
  filter((category != 'yes') & (category != 'none')) %>%
  group_by(model_type, category) %>%
  summarize(n = n(), mean_probs = mean(probs)) %>%
  mutate(prop = n / sum(n)) %>%
  mutate(answerType = factor(category, levels=answerOrder))

df_samples_byModel_byItem_summary <- nm_samples_categorized %>%
  filter((category != 'yes') & (category != 'none')) %>%
  group_by(model_name, is_few_shot, itemName, category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  mutate(answerType = factor(category, levels=answerOrder))

df_samples_byItem_summary <- nm_samples_categorized %>%
  filter((category != 'yes') & (category != 'none')) %>%
  group_by(itemName, category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  mutate(answerType = factor(category, levels=answerOrder))
cat("Proportion of response categories by vignette:")
df_samples_byItem_summary

df_samples_global_summary <- nm_samples_categorized %>%
  filter((category != 'yes') & (category != 'none')) %>%
  group_by(category) %>%
  summarize(n = n()) %>%
  mutate(prop = n / sum(n)) %>%
  mutate(answerType = factor(category, levels=answerOrder))
cat("Global response category proportions:")
df_samples_global_summary
```

## Plots

The first plot shows global response proportions, averaged across models and vignettes.

```{r, echo=FALSE, fig.height=4, fig.width=6}
df_samples_global_summary %>%
  ggplot(aes(x = answerType, fill = answerType, y = prop)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylab("Global proportion of answer type") +
  xlab("Answer type")
```

The next plot displays proportions of different responses in generative models (LMs) vs. extractive QA models:
```{r, echo=FALSE, fig.height=4, fig.width=6}
bar.width <- 0.8
df_samples_byModelType_summary %>% ungroup() %>%
  mutate(model_type = factor(model_type, levels = c("extractive", "generative"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = prop, pattern = model_type)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(extractive = "stripe", generative = "none")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Model type") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  ggtitle("Response type proportions by model type (extractive vs generative)")
```

The next plot displays proportions of different responses from different models, averaging across one-shot and zero-shot LMs.

```{r, echo=FALSE, fig.height=15, fig.width=12}
bar.width <- 0.8
df_samples_byModel_summary %>% ungroup() %>%
  ggplot(aes(x = answerType, fill = answerType, y = prop)) +
  geom_col() +
  facet_wrap( model_name ~ . , ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  xlab("Answer type") +
  ylab("Response type proportion")
```


Below, we explore the idea of **weighting the proportions** of different response types sampled deterministically from the different models by the probability of the respective samples under the given model. Since the samples taken were the top 5 predictions, this is to account for potential differences in the model's propensity to generate a given response type (as, e.g., top 1 vs top 5 response).

```{r, fig.height=15, fig.width=12}
df_samples_byModel_summary_weighted <- df_samples_byModel_summary %>% ungroup() %>%
  mutate(
    prop_weighted = prop * mean_probs
  ) %>% group_by(model_name) %>%
  mutate(
    sum_props = sum(prop_weighted),
    prop_weighted_renormalized = prop_weighted / sum_props
  )

df_samples_byModel_weighting_comparison <- df_samples_byModel_summary_weighted %>% 
  select(model_name, answerType, prop_weighted_renormalized) %>%
  rename("prop" = "prop_weighted_renormalized") %>%
  mutate(weighted = TRUE) %>%
  rbind(., df_samples_byModel_summary %>% select(model_name, answerType, prop) %>% mutate(weighted = FALSE))

bar.width <- 0.8
df_samples_byModel_weighting_comparison %>% ungroup() %>%
  mutate(weighted = factor(weighted, levels = c(TRUE, FALSE))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = prop, pattern = weighted)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(`TRUE` = "stripe", `FALSE` = "none")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Proportions weighted by probs?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  facet_wrap( model_name ~ . , ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  ggtitle("Response type proportions weighted by response probability and renormalized")
```

The next plot displays proportions of different responses from different models, differentiating between one-shot and zero-shot LMs.

```{r, echo=FALSE, fig.height=15, fig.width=12}
bar.width <- 0.8
df_samples_byModel_byContext_summary %>% ungroup() %>%
  ggplot(aes(x = answerType, fill = answerType, y = prop, pattern = is_few_shot)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(`TRUE` = "stripe", `FALSE` = "none")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "One shot context?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  facet_wrap( model_name ~ . , ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) 
```

The next plot displays proportions of different responses from models by-vignette. 

```{r, echo=FALSE, fig.height=28, fig.width=12}
df_samples_byItem_summary %>% 
  ggplot(aes(x = answerType, fill = answerType, y = prop)) +
  geom_col() +
  facet_wrap( itemName ~ . , ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  ylab("Proportion of answer types") +
  xlab("Answer type") 
  
```


### Comparing neural and human responses

Below, the response type categories sampled from models and collected in the free production human experiment are plotted against each other.

First, the global proportions are compared.

```{r, echo=FALSE, fig.height=4, fig.width=6}
df_samples_human_global_summary <- df_samples_global_summary %>% 
  mutate(model_name = "neural_models") %>%
  select(-n, -category) %>%
  rbind(., df_human_global)

df_samples_human_global_summary %>% 
  mutate(model_name = factor(model_name, levels = c("human", "neural_models"))) %>%
  ggplot(., aes(x = answerType, y = prop, fill = answerType, pattern = model_name)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(human = "stripe", neural_models = "none")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Human data?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  theme(legend.text = element_text(size=7),
        legend.title = element_text(size=7)
        )+
  ylab("Global proportion of answer type") +
  xlab("Answer type") +
  ggtitle("Response type proportions of human data vs\nneural model samples")
```

Next, by-vignette results can be found below.

```{r, echo=FALSE, fig.height=28, fig.width=12}
df_samples_human_byItem_summary <- df_samples_byItem_summary %>% 
  select(-n, -category) %>%
  mutate(model_name = "neural_models") %>%
  rbind(., df_human)

bar.width <- 0.9

df_samples_human_byItem_summary %>% 
  mutate(model_name = factor(model_name, levels = c("human", "neural_models"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = prop, pattern = model_name)) +
  geom_col_pattern(position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
           ) +
  scale_pattern_manual(values = c(human = "stripe", neural_models = "none")) +
  facet_wrap( itemName ~ . , ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Human data?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  ggtitle("Response type proportions of human data vs neural model samples by-vignette")
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=5, eval=FALSE}
# Finally, compare responses manually retrieved from ChatGPT, with and without one-shot prompting.
chatGPT_oneshot <- read_csv("../code/results/categorized_responses/chatGPT-samples-oneShotLearer-cogsci_categorized.csv") %>%
  mutate(is_one_shot = TRUE)
chatGPT_zeroshot <- read_csv("../code/results/categorized_responses/chatGPT-samples-zeroShotLearer-cogsci_categorized.csv") %>%
  mutate(is_one_shot = FALSE)

chatGPT <- rbind(chatGPT_oneshot, chatGPT_zeroshot) %>%
  mutate(answerType = factor(category, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn", "other")))

chatGPR_summary <- chatGPT %>% group_by(answerType, is_one_shot) %>%
  summarize(n = n()) %>%
  mutate(prop = n / 30)

chatGPR_summary %>% 
  mutate(is_one_shot = factor(is_one_shot, levels = c(TRUE, FALSE))) %>%
  ggplot(., aes(x = answerType, y = prop, fill = answerType, pattern = is_one_shot)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(`TRUE` = "stripe", `FALSE` = "none")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "One shot context?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Global proportion of answer type") +
  xlab("Answer type") +
  ggtitle("Response type proportions of ChatGPT with vs without one-shot prompting")
```

### Model probabilities

Below, the scores assigned to provided answer types are compared to the proportions of those answer types generated by the language models during sampling. Whenever the responses sampled from the model were difficult to classify due to half-generated phrases (like "kick" instead of kickboxing), they were categorized as "other" responses. 
The comparison is by model type. 

First, GPT-3 results are compared.
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=4 }
# add gpt-3 results
gpt3_oneshot <- read_csv("../code/results/GPT3-davinci-003-predictions-overinfo-oneShotLearer-cogsci.csv")
gpt3_oneshot_ppl <- read_csv("../code/results/GPT3-davinci-003-predictions-overinfo-oneShotLearer-cogsci-long-prob.csv") %>% 
  mutate(answerType = rep(c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'fullList'), 30),
         answerType = factor(answerType, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn") ),
         model_name = "gpt3_oneShot"
         ) %>%
  select(itemName, answerType, answer_type_prob_avg, answer_type_ppl, answer_type_length_norm_prob)
gpt3_oneshot_long <- gpt3_oneshot %>%
  pivot_longer(cols = c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'fullList'), names_to = 'answerType', values_to = 'answer_type_prob') %>%
  mutate(answerType = factor(answerType, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn") ),
         model_name = "gpt3_oneShot") %>%
  left_join(., gpt3_oneshot_ppl, by=c("itemName", "answerType"))

gpt3_oneshot_summary <- gpt3_oneshot_long %>% group_by(answerType, model_name) %>%
  summarize(answer_type_prob = mean(answer_type_prob),
            answer_type_prob_avg = mean(answer_type_prob_avg),
            #answer_type_length_norm_prob = mean(answer_type_length_norm_prob),
            answer_type_ppl = mean(answer_type_ppl)
            )

gpt3_zeroshot <- read_csv("../code/results/GPT3-davinci-003-predictions-overinfo-zeroShotLearer-cogsci.csv")
gpt3_zeroshot_ppl <- read_csv("../code/results/trials_LLMs_GPT3-davinci-003-predictions-overinfo-zeroShotLearer-cogsci-long_prob.csv") %>% 
  mutate(answerType = rep(c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'fullList'), 30),
         answerType = factor(answerType, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn") ),
         model_name = "gpt3_zeroShot"
         ) %>%
  select(itemName, answerType, answer_type_prob_avg, answer_type_ppl, answer_type_length_norm_prob)

gpt3_zeroshot_long <- gpt3_zeroshot %>%
  pivot_longer(cols = c('taciturn', 'competitor', 'sameCategory', 'otherCategory', 'fullList'), names_to = 'answerType', values_to = 'answer_type_prob') %>%
  mutate(answerType = factor(answerType, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn") ),
         model_name = "gpt3_zeroShot") %>%
  left_join(., gpt3_zeroshot_ppl, by = c("itemName", "answerType"))

gpt3_zeroshot_summary <- gpt3_zeroshot_long %>% group_by(answerType, model_name) %>%
  summarize(answer_type_prob = mean(answer_type_prob),
            answer_type_prob_avg = mean(answer_type_prob_avg),
            #answer_type_length_norm_prob = mean(answer_type_length_norm_prob),
            answer_type_ppl = mean(answer_type_ppl)
            )

gpt3_summary <- rbind(gpt3_zeroshot_summary, gpt3_oneshot_summary)
gpt3_summary %>%
  mutate(model_name = factor(model_name, levels = c("gpt3_zeroShot", "gpt3_oneShot"))) %>%
  ggplot(., aes(x = answerType, y = answer_type_prob, fill = answerType, pattern = model_name)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(gpt3_oneShot = "stripe", gpt3_zeroShot = "none")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "One shot context?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Global proportion of answer type") +
  xlab("Answer type") +
  ggtitle("Response type probabilities of GPT-3 davinci-003 with vs without one-shot prompting")

```

Next, scores are plotted by-model.

```{r, echo=FALSE, fig.height=15, fig.width=12}
lm_scores_byModel <- lm_scores %>% select(answer_type, answer_type_prob, answer_type_prob_avg, answer_type_ppl, model_name) %>%
  mutate(answerType = factor(answer_type, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn"))) %>%
  group_by(answerType, model_name) %>%
  summarize(answer_type_prob = mean(answer_type_prob),
            answer_type_prob_avg = mean(answer_type_prob_avg),
            answer_type_ppl = mean(answer_type_ppl)
            ) 

# append gpt-3 results
lm_scores_byModel <- lm_scores_byModel %>%
  rbind(., gpt3_summary) 

lm_scores_byModel %>%
  ggplot(aes(x = answerType, fill = answerType, y = answer_type_prob)) +
  geom_col() +
  facet_wrap( model_name ~ . , ncol = 3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  ylab("Probability of answer types") +
  xlab("Answer type") +
  ggtitle("Propabilities of answer types under different LMs")

```

Next, probabilities computed for single response options are compared to average probabilities computed over all permutations of alternatives in the response sentences and generation proportions.

```{r, echo=FALSE}
df_samples_byModel_byContext_gpt3 <- df_samples_byModel_byContext_summary %>% filter(model_name == "gpt3-davinci-003") %>% 
  mutate(model_name = ifelse(is_few_shot == TRUE, "gpt3_oneShot", "gpt3_zeroShot"))

df_samples_byModel_summary_forComp <- df_samples_byModel_summary %>% ungroup() %>%
  filter(model_name != "gpt3-davinci-003") %>%
           #   mutate(model_name = ifelse(model_name == "gpt3-davinci-003", "gpt3_oneShot", model_name)) %>%
              select(model_name, answerType, prop) %>% #filter(answerType != "other") %>%
  rbind(., df_samples_byModel_byContext_gpt3 %>% ungroup() %>% select(model_name, answerType, prop))

df_samples_byModel_summary_forComp <- df_samples_byModel_summary_forComp %>% group_by(model_name) %>%
  mutate(propSum = sum(prop),
         prop = prop / propSum
         ) %>% ungroup()

lm_scores_byModel_wProduction <- lm_scores_byModel %>%
  left_join(., df_samples_byModel_summary_forComp,
            by = c("model_name", "answerType")
            ) %>%
  mutate(prop = ifelse(is.na(prop), 0, prop)) %>%
  left_join(., df_human_global %>% mutate(human_prop = prop) %>% select(-model_name, -prop), by = "answerType")

```

```{r, echo=FALSE, fig.height=15, fig.width=16}
lm_scores_byModel_wProduction %>%
  filter(!(is.na(answer_type_prob_avg))) %>%
  pivot_longer(cols = c("answer_type_prob", "answer_type_prob_avg", "prop", "human_prop"), names_to = "computation", values_to = "probs") %>%
  mutate(computation = factor(computation, levels = c("human_prop", "answer_type_prob", "answer_type_prob_avg", "prop"), labels = c("human", "singleOption", "allPermutations", "productionProp"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = probs, pattern = computation)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(human = "none", singleOption = "stripe",  allPermutations = "crosshatch", productionProp = "circle")) +
  facet_wrap( model_name ~ . , ncol = 3) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Number of scored sentences / production") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  ggtitle("Propabilities of answer types / sampling proportions under different LMs and different probability computations")

```


Compare scores assigned to original response types and short responses without the taciturn prefix.

```{r, echo=FALSE, fig.height=15, fig.width=16}
lm_scores_short <- data.frame()
all_short_score_files <- list.files("../code/results/lm_probs/short_responses/", pattern="*.csv", full.names = TRUE)

for (f in all_short_score_files) {
  print(f)
  df <- read_csv(f)
  # rename GPT cols
  if ("predictions_cleaned" %in% colnames(df)) {
    df <- df %>% select(-predictions) %>%
      rename(., c('predictions' = 'predictions_cleaned'))      
  } 
  if ("answer_type" %in% colnames(df)) {
    df <- df %>% 
      rename(., c('answerType' = 'answer_type'))      
  } 
  df <- df %>% 
    select(itemName, answerType, model_name, answer_type_prob_avg, answer_type_ppl, answer_type_length_norm_prob) %>%
    rename(., c('answer_type_prob_avg_short' = 'answer_type_prob_avg', 'answer_type_ppl_short' = 'answer_type_ppl', 'answer_type_length_norm_prob_short' = 'answer_type_length_norm_prob')
                      ) 
    
  lm_scores_short <- rbind(lm_scores_short, df)
}
head(lm_scores_short)

lm_scores_short_summary <- lm_scores_short %>% 
  mutate(answer_type_length_norm_prob_short = ifelse(is.na(answer_type_length_norm_prob_short), 0, answer_type_length_norm_prob_short)) %>%
  group_by(model_name, answerType) %>%
  summarize(answer_type_prob_avg_short = mean(answer_type_prob_avg_short),
            answer_type_length_norm_prob_short = mean(answer_type_length_norm_prob_short),
            answer_type_ppl_short = mean(answer_type_ppl_short)
            ) 

lm_scores_byModel_wProduction_wShort <- lm_scores_byModel_wProduction %>%
  left_join(., lm_scores_short_summary, by = c("model_name", "answerType"))


lm_scores_byModel_wProduction_wShort %>% 
  filter(!(is.na(answer_type_prob_avg))) %>%
  pivot_longer(cols = c("answer_type_prob", "answer_type_prob_avg", "answer_type_prob_avg_short", "prop", "human_prop"), names_to = "computation", values_to = "probs") %>%
  mutate(computation = factor(computation, levels = c("human_prop", "answer_type_prob", "answer_type_prob_avg", "answer_type_prob_avg_short", "prop"), labels = c("human", "singleOption", "allPermutations", "shortResps", "productionProp"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = probs, pattern = computation)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
 scale_pattern_manual(values = c(human = "none", singleOption = "stripe",  allPermutations = "crosshatch", shortResps = "polygon_tiling", productionProp = "circle")) +
  facet_wrap( model_name ~ . , ncol = 3) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Number of scored sentences / production") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  ggtitle("Propabilities of answer types / sampling proportions under different LMs and different probability computations")

```

For comparison, look at qualitative pattern of perplexities.

```{r, echo=FALSE, fig.height=15, fig.width=12}
lm_scores_byModel_wProduction_wShort %>%
  filter(!(is.na(answer_type_ppl) & is.na(answer_type_ppl_short))) %>%
  pivot_longer(cols = c("answer_type_ppl", "answer_type_ppl_short"), names_to = "computation", values_to = "probs") %>%
  mutate(computation = factor(computation, levels = c("answer_type_ppl_short", "answer_type_ppl"), labels = c("PPL_short", "PPL"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = probs, pattern = computation)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(PPL = "none", PPL_short = "stripe")) +
  facet_wrap( model_name ~ . , ncol = 3, scales = "free") +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Number of scored sentences / production") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  ggtitle("Perplexities of answer types under different LMs")

```

For the paper, we average the results from short and long response probs and compare them to the models' sample proportions and human performance:
```{r, echo=FALSE, fig.width=16, fig.height=15}
lm_scores_byModel_wProduction_avg <- lm_scores_byModel_wProduction_wShort %>% 
  filter(!(is.na(answer_type_prob_avg))) %>%
  mutate(answer_type_avg = (answer_type_prob_avg + answer_type_prob_avg_short)/2,
         answerType = factor(answerType, levels=c("competitor","sameCategory", "otherCategory", "fullList", "other", "taciturn"))
         ) 

lm_scores_byModel_wProduction_avg %>%
  pivot_longer(cols = c("answer_type_avg", "prop", "human_prop"), names_to = "computation", values_to = "probs") %>%
  mutate(computation = factor(computation, levels = c("human_prop", "answer_type_avg", "prop"), labels = c("human", "answerTypeProbLM", "samplePropLM"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = probs, pattern = computation)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
 scale_pattern_manual(values = c(human = "none", answerTypeProbLM = "stripe", samplePropLM = "circle")) +
  facet_wrap( model_name ~ . , ncol = 3) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Type of data") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 14)) +
  theme(panel.spacing = unit(3, "lines")) +
  ggtitle("Propabilities of answer types / sampling proportions under different LMs")


```

For comparison, plot the length normalized probabilities retrieved from the models. The expectation is that the qualitative pattern matches the pattern of the PPLs by model.
```{r, echo=FALSE, fig.height=15, fig.width=12}
gpt3_long <- gpt3_zeroshot_long %>% rbind(., gpt3_oneshot_long)

p1 <- gpt3_long %>% group_by(answerType, model_name) %>%
  summarize(answer_type_length_norm_prob = mean(answer_type_length_norm_prob)) %>%
  ggplot(aes(x = answerType, fill = answerType, y = answer_type_length_norm_prob)) +
  geom_col() +
  facet_wrap( model_name ~ . , ncol = 1) +
  labs(x = "Answer type", y = "Response type probability") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) 

p2 <- gpt3_long %>%
  group_by(answerType, model_name) %>%
  summarize(answer_type_ppl = mean(answer_type_ppl)) %>%
  ggplot(aes(x = answerType, fill = answerType, y = answer_type_ppl)) +
  geom_col() +
  facet_wrap( model_name ~ . , ncol = 1) +
  labs(x = "Answer type", y = "Response type perplexity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines"))

gridExtra::grid.arrange(p1, p2, ncol=2 )
```
```{r, echo=FALSE, fig.height=25, fig.width=12}
# same for short response scoring
p1 <- lm_scores_byModel_wProduction_wShort %>%
  filter(!(is.na(answer_type_ppl) & is.na(answer_type_ppl_short))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = answer_type_ppl_short)) +
  geom_col() +
  facet_wrap( model_name ~ . , ncol = 1, scales = "free") +
  labs(x = "Answer type", y = "Response type perplexity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) 

p2 <- lm_scores_byModel_wProduction_wShort %>%
  filter(!(is.na(answer_type_ppl) & is.na(answer_type_ppl_short))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = answer_type_prob_avg_short)) +
  geom_col() +
  facet_wrap( model_name ~ . , ncol = 1, scales = "free") +
  labs(x = "Answer type", y = "Response type probability") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) 


gridExtra::grid.arrange(p2, p1, ncol=2 )
```

Compare the samples and the probabilities assigned by the models to human data (across models and vignettes).

```{r, echo=FALSE, fig.height=4, fig.width=6}
lm_scores_global <- lm_scores %>% select(answer_type, answer_type_prob, model_name) %>%
  mutate(answerType = factor(answer_type, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn"))) %>%
  group_by(answerType) %>%
  summarize(answer_type_prob = mean(answer_type_prob)) 
  
df_human_lm_scores <- lm_scores_global %>%
  mutate(model_name = "neural_models") %>% 
  select(answerType, answer_type_prob, model_name) %>%
  rbind(., df_human_global %>% rename("answer_type_prob" = "prop"))

df_human_lm_scores %>%
  mutate(model_name = factor(model_name, levels = c("human", "neural_models"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = answer_type_prob, pattern = model_name)) +
  geom_col_pattern(position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
           ) +
  scale_pattern_manual(values = c(human = "stripe", neural_models = "none")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  labs(x = "Answer type", y = "Response type proportion / P under LM", pattern = "Human data?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  ggtitle("Response type proportions of human data vs neural model scores of human response types")

```

Below, scores by-vignette are compared (across models).

```{r, echo=FALSE, fig.height=28, fig.width=12}
lm_scores_byItem <- lm_scores %>% select(answer_type, answer_type_prob, itemName, model_name) %>%
  mutate(answerType = factor(answer_type, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn"))) %>%
  group_by(answerType, itemName) %>%
  summarize(answer_type_prob = mean(answer_type_prob)) 
  
df_human_lm_scores_byItem <- lm_scores_byItem %>%
  mutate(model_name = "neural_models") %>% 
  select(answerType, answer_type_prob, model_name, itemName) %>%
  rbind(., df_human %>% rename("answer_type_prob" = "prop"))

df_human_lm_scores_byItem %>%
  mutate(model_name = factor(model_name, levels = c("human", "neural_models"))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = answer_type_prob, pattern = model_name)) +
  geom_col_pattern(position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
           ) +
  scale_pattern_manual(values = c(human = "stripe", neural_models = "none")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  facet_wrap(itemName ~ ., ncol = 4) +
  labs(x = "Answer type", y = "Response type proportion / P under LM", pattern = "Human data?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  ggtitle("Response type proportions of human data vs neural model scores of human response types by item")

```

Compare the scores of different models to their proportion predictions:

```{r, echo=FALSE, fig.height=8, fig.width=6, warning=FALSE, message=FALSE}
df_models_scores_samples <- df_samples_byModel_byItem_summary %>% ungroup() %>%
  filter(is_few_shot == FALSE) %>% select(model_name, itemName, answerType, prop, -is_few_shot) %>%
  inner_join(., lm_scores %>% select(itemName, model_name, answer_type_prob, answer_type) %>%
  mutate(answerType = factor(answer_type, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn", "other"))) %>% select(-answer_type))

df_models_scores_samples %>%
  ggplot(aes(x = answer_type_prob, fill = answerType, color = answerType, y = prop)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  labs(x = "Answer type probability under LM", y = "Sampled response type proportion") +
  facet_wrap(answerType ~ ., ncol = 2)
```

A comparison of human data against average probabilities / samples proportions of the models across models is shown below:
```{r, echo=FALSE, fig.height=4, fig.width=6}
lm_scores_byModel_wProduction_avg_summary <- lm_scores_byModel_wProduction_avg %>% 
  mutate(model_name = "model_prob") %>%
  group_by(answerType, model_name) %>%
  summarize(prop = mean(answer_type_avg))


df_samples_human_global_summary %>% 
  rbind(., lm_scores_byModel_wProduction_avg_summary) %>%
  mutate(model_name = factor(model_name, levels = c("human", "neural_models", "model_prob"), labels = c("human", "neural_sample_props", "neural_probs"))) %>%
  ggplot(., aes(x = answerType, y = prop, fill = answerType, pattern = model_name)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(human = "none", neural_sample_props = "stripe", neural_probs = "circle")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Data type") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  theme(legend.text = element_text(size=6),
        legend.title = element_text(size=6)
        )+
  ylab("Global proportion of answer type") +
  xlab("Answer type")

```




## Stats

Below, the KL-divergence of between global human response type proportions (renormalized after excluding 'other' responses) and probabilities assigned by the models are compared. Note the implicit assumption that the proportions of different response types produced by humans are the probabilities of producing that particular type given the context.

```{r, message=FALSE, warning=FALSE}
compute_kl <- function(P, Q) {
  tryCatch(
    expr = {
      df <- rbind(P, Q)
      kl <- philentropy::KL(df)
      return(kl)
    },
    error = function(e){
      print("Numerical precision error")
      print(e)
    }
  )
}

df_human_global_renorm <- df_human_global %>% filter(answerType != "other") %>%
  mutate(answer_type_prob = prop / (df_human_global %>% filter(answerType != "other") %>% pull(prop) %>% sum()))
# second is the reference dist
model_human_dists <- rbind(lm_scores_global %>% pull(answer_type_prob), df_human_global_renorm %>% pull(answer_type_prob)) 
kl <- philentropy::KL(model_human_dists)
cat("KL divergence between human data and probs of responses averaged over models: ", kl)
```

Compute KLs between response proportions, probabilities by model and by vignette (for some cases numerical precision errors don't allow for computation of the KL, in that case respective / model vigntte are skipped).

```{r, warning=FALSE, message=FALSE, echo=FALSE}
cat("KL between GPT-3 one shot probs and human data ")
compute_kl(gpt3_oneshot_summary %>% pull(answer_type_prob), df_human_global_renorm %>% pull(answer_type_prob))[[1]]
cat("KL between GPT-3 zero shot probs and human data ")
compute_kl(gpt3_zeroshot_summary %>% pull(answer_type_prob), df_human_global_renorm %>% pull(answer_type_prob))[[1]]

```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
all_score_models <- lm_scores_byModel %>% pull(model_name) %>% unique()
# compare model probs to human data
df_kls_probs_vs_human_byModel <- data.frame(model_name=character(), KL=double())

for (m in all_score_models) {
  df_model_renorm <- lm_scores_byModel %>% filter(model_name == m)
  kl <- compute_kl(df_model_renorm %>% pull(answer_type_prob), df_human_global_renorm %>% pull(answer_type_prob))
  df_kls_probs_vs_human_byModel <- df_kls_probs_vs_human_byModel %>% add_row(model_name = m, KL = kl[[1]])
}
cat("KLs between probabilities of different responses computed under different models and human response proportions ")
df_kls_probs_vs_human_byModel

# compare model sample proportions to human data
df_kls_props_vs_human_byModel <- data.frame(model_name=character(), KL=double())
all_sample_models <- df_samples_byModel_summary %>% pull(model_name) %>% unique()

for (m in all_sample_models) {
  print(m)
  df_model_renorm <- df_samples_byModel_summary %>% filter(model_name == m)
  #print(df_model_renorm %>% pull(prop) %>% sum())
  #print(df_human_global_renorm %>% pull(answer_type_prob) %>% sum())
  kl <- compute_kl(df_model_renorm %>% .$prop/sum(df_model_renorm %>% pull(prop)), df_human_global_renorm %>% .$answer_type_prob/sum(df_human_global_renorm %>% pull(answer_type_prob)))
  if (is.numeric(kl[[1]])) {
      df_kls_props_vs_human_byModel <- df_kls_props_vs_human_byModel %>% add_row(model_name = m, KL = kl[[1]])
  }
} 
cat("KLs between different response type proportions of different models and human response proportions ")
df_kls_props_vs_human_byModel

df_kls_props_vs_human_byItem <- data.frame(itemName=character(), KL=double())
all_items <- df_samples_byItem_summary %>% pull(itemName) %>% unique()

for (m in all_items) {
  print(m)
  df_model_renorm <- df_samples_byItem_summary %>% filter(itemName == m)
  df_human_item <- df_human %>% filter(itemName == m)
  #print(df_model_renorm %>% pull(prop) %>% sum())
  #print(df_human_item %>% pull(prop) %>% sum())
  kl <- compute_kl(df_model_renorm %>% pull(prop), df_human_item %>% pull(prop))
  if (is.numeric(kl[[1]])) {
    df_kls_props_vs_human_byItem <- df_kls_props_vs_human_byItem %>% add_row(itemName = m, KL = kl[[1]])
  }
}
cat("KLs between different response type proportions by item between neural model and human response proportions ")
df_kls_props_vs_human_byItem
```

Compute a multinomial regression with a main effect of human vs model data, regressing the response category against an intercept and a main effect of data source, and random by-item intercepts. Competitor responses are coded as the reference level.

```{r, message=FALSE, warning=FALSE, error=FALSE}
df_human_raw <- read_csv("data/results_QA-overinfo-freeTyping-cogsci_full_anonymized_categorized.csv") %>% 
  filter(category != 'yes', !(submission_id %in% c(4608, 4690, 4687, 4733, 4763))) %>% 
  mutate(answerType = factor(category, levels = answerOrder),
         model_name = "human"
         ) %>% 
  select(itemName, answerType, model_name)

df_samples_human_raw <- nm_samples_categorized %>% 
  filter((category != "yes") & (category != "none")) %>%
  mutate(
    answerType = factor(category, levels = answerOrder),
    model_name = "neural"
  ) %>%
  select(itemName, answerType, model_name) %>%
  rbind(., df_human_raw) %>%
  mutate(
    model_name = factor(model_name, levels = c("human", "neural"))
  )

contrasts(df_samples_human_raw$answerType)
contrasts(df_samples_human_raw$model_name)

model <- brm(answerType ~ 1 + model_name + (1 | itemName), 
             data = df_samples_human_raw,
             family = "categorical",
             iter = 4000
             )
summary(model)
```

Extract contrasts by response type between human data and neural model data (i.e., probability that proportion of given response type is larger in human than in neural model data):

```{r}
model_posteriors <- model %>% spread_draws(b_musameCategory_Intercept, b_muotherCategory_Intercept, b_mufullList_Intercept, b_mutaciturn_Intercept, b_muother_Intercept, b_musameCategory_model_nameneural, b_muotherCategory_model_nameneural, b_mufullList_model_nameneural, b_mutaciturn_model_nameneural, b_muother_model_nameneural) %>%
  mutate(
    sameCategory = b_musameCategory_Intercept - b_musameCategory_model_nameneural,
    otherCategory = b_musameCategory_Intercept - b_muotherCategory_model_nameneural,
    fullList = b_mufullList_Intercept - b_mufullList_model_nameneural,
    taciturn = b_mutaciturn_Intercept - b_mutaciturn_model_nameneural,
    other = b_muother_Intercept - b_muother_model_nameneural
  )

model_posteriors %>% select(sameCategory, otherCategory, fullList, taciturn, other) %>%
  gather(key, val) %>%
  group_by(key) %>%
  summarize(
    '|95%' = quantile(val, probs = c(0.025, 0.975))[[1]],
    'mean'  = mean(val),
    '95%|' = quantile(val, probs = c(0.025, 0.975))[[2]],
    prob_gt_0 = mean(val > 0)*100,
    prob_lt_0 = mean(val < 0)*100
  ) -> model_posteriors_summary

model_posteriors_summary

# TODO: determine the prob that competitor proportion in human data larger than in neural data
```

Compute p-value of human data under model data: Chi-square tests of human response proportions vs model probabilities and response proportions (assumed to be ground truth).

```{r, warning=FALSE, message=FALSE}
# assumed order of counts / proportions: comp, sameCat, otherCat, fullList, taciturn (excluding other)
human_counts <- df_human_raw %>% filter(answerType != "other") %>% group_by(answerType) %>%
  summarize(count = n()) 

compute_p <- function(human_counts, name, models_df, type) {
  cats <- c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn")
  
  if (type == "samples") {
    df <- models_df %>% filter(model_name == name) %>% select(answerType, prop) %>% filter(answerType != "other")
  } else {
    df <- models_df %>% filter(model_name == name) %>% select(answerType, answer_type_prob) %>%
      rename("prop" = "answer_type_prob") %>% filter(answerType != "other")
  }
  answer_types <- df$answerType
  # pad missing category with epsilon and renormalize
  if (answer_types %>% length() < 5) {
    for (a in setdiff(cats, answer_types)) {
      df <- df %>% ungroup() %>%
      add_row(answerType = a, prop = 0.01)
    }
  }
  df <- df %>% ungroup() %>% mutate(
        answerType = factor(answerType, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "taciturn")),
        sum_prop = sum(prop),
        prop = prop / sum_prop
      ) 
  res <- chisq.test(x = human_counts, p = df %>% pull(prop))
  return(res$p.value)
}

compute_p(human_counts %>% pull(count), "chatGPT", df_samples_byModel_summary, "samples")

# define df 
df_pvals_models_vs_humans <- data.frame(model_name=character(), type=character(), p=double())
# iterate over model samples
for (m in all_sample_models) {
  p <- compute_p(human_counts %>% pull(count), m, df_samples_byModel_summary, "samples")
  df_pvals_models_vs_humans <- df_pvals_models_vs_humans %>%
    add_row(model_name=m, type="samples", p=p)
}
# iterate over scores
for (m in all_score_models) {
  p <- compute_p(human_counts %>% pull(count), m, lm_scores_byModel, "prob")
  df_pvals_models_vs_humans <- df_pvals_models_vs_humans %>%
    add_row(model_name=m, type="prob", p=p)
}
cat("P values under Chi square tests of human data against probabilities / sample proportions of neural models ")
df_pvals_models_vs_humans
```

```{r}
all(df_pvals_models_vs_humans %>% pull(p) < 0.05)
```

## Experiment 2

Below, the same computations and plots are provided for stimuli / samples from experiment 2.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#lm_scores_e2 <- data.frame()
#all_score_files_e2 <- list.files("../code/results/experiment2", pattern="^GPT3-davinci-003-predictions-(.)*csv", full.names = TRUE)
lm_scores_e2 <- read_csv("../data_paper_neural/e2_SA_model_probs.csv")
for (f in all_score_files_e2) {
  print(f)
  df <- read_csv(f)
  # rename GPT cols
  if ("predictions_cleaned" %in% colnames(df)) {
    df <- df %>% select(-predictions) %>%
      rename(., c('predictions' = 'predictions_cleaned'))      
  } 
  lm_scores_e2 <- rbind(lm_scores_e2, df)
}
head(lm_scores_e2)
#lm_scores_e2 %>% write_csv("../code/results/experiment2/GPT3-davinci-003-predictions-overinfo-zeroShotLearner-pilot4.csv")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#lm_predictions_e2 <- data.frame()
#all_sample_files_e2 <- list.files("../code/results/experiment2", pattern="^GPT3-davinci-003-samples-(.)*csv", full.names = TRUE)
lm_predictions_e2 <- read_csv("../data_paper_neural/QA_neural_models_categorized_samples_promptE2.csv")

for (f in all_sample_files_e2) {
  print(f)
  df <- read_csv(f)
  # rename GPT cols
  if ("predictions_cleaned" %in% colnames(df)) {
    df <- df %>% select(-predictions) %>%
      rename(., c('predictions' = 'predictions_cleaned'))      
  } 
  lm_predictions_e2 <- rbind(lm_predictions_e2, df)
}
head(lm_predictions_e2)
#lm_predictions_e2 %>% write_csv("../code/results/experiment2/GPT3-davinci-003-samples-zeroShotLearner-m64-pilot4.csv")
```

```{r, echo=FALSE, fig.width=8, fig.height=6}
lm_scores_e2 <- lm_scores_e2 %>%
  mutate(context_nr = case_when(
    itemName == 'sugar-coffee' ~ 'context1',
    itemName == 'sugar-baking' ~ 'context2',
    itemName == 'blanket-sleepover' ~ 'context1',
    itemName == 'blanket-transportation' ~ 'context2',
    itemName == 'chair-repair' ~ 'context1',
    itemName == 'chair-party' ~ 'context2',
    itemName == 'box-muffins' ~ 'context1',
    itemName == 'box-bbq' ~ 'context2',
    itemName == 'handkerchief-injury' ~ 'context1',
    itemName == 'handkerchief-sauce' ~ 'context2',
    itemName == 'broom-renovation' ~ 'context1',
    itemName == 'broom-snow' ~ 'context2',
    itemName == 'sweatpants-sleepover' ~ 'context1',
    itemName == 'sweatpants-spill' ~ 'context2',
    itemName == 'umbrella-summer' ~ 'context1',
    itemName == 'umbrella-rain' ~ 'context2',
    itemName == 'postit-board' ~ 'context1',
    itemName == 'postit-reading' ~ 'context2',
    itemName == 'pot-garden' ~ 'context1',
    itemName == 'pot-jam' ~ 'context2',
    itemName == 'towel-hair' ~ 'context1',
    itemName == 'towel-clothes' ~ 'context2',
    itemName == 'paper-bbq' ~ 'context1',
    itemName == 'paper-hamster' ~ 'context2',
    itemName == 'candy-halloween' ~ 'context1',
    itemName == 'candy-dessert' ~ 'context2',
    itemName == 'bottle-plants' ~ 'context1',
    itemName == 'bottle-water' ~ 'context2',
    TRUE ~ itemName
  ),
  answer_type = factor(answer_type, levels = c("competitor", "mostSimilar", "sameCategory", "otherCategory", "fullList", "taciturn"))
  )

lm_scores_e2_summary <- lm_scores_e2 %>% group_by(answer_type) %>%
  summarize(mean_prob = mean(answer_type_prob_avg),
            mean_ppl = mean(answer_type_ppl))

# add human data
df <- read_csv("data/results_105_QA-overinfo-contextDependent-freeTyping-pilot4_38_categorized.csv")
df_attention <- df %>% filter(trial_type == "filler") %>% rowwise() %>%
  mutate(passed_trial = grepl(tolower(correct_response), tolower(answer))) 

df_attention_fail <- df_attention %>% group_by(submission_id) %>% 
  mutate(passed_subj = sum(passed_trial) > 0)

# participants who failed attention checks 
subj_id_attention_fails <- df_attention_fail %>% filter(passed_subj == FALSE) %>% pull(submission_id) %>% unique()
df_clean_main <- df %>% filter(!(submission_id %in% subj_id_attention_fails)) %>%
  filter(trial_type == "main", category != "yes") %>%
  mutate(category = factor(category))

e3_pilot_vignettes <- read_csv("../experiments/contextSensitive_free_production/trials/trials_split_cogsci_pilot4.csv") %>% 
  select(itemName, settingName, itemQuestion, competitor, mostSimilar, sameCategory,otherCategory)
e3_pilot_wVignette <- left_join(df_clean_main, e3_pilot_vignettes, by = c("itemName", "settingName")) 
e3_summary <- df_clean_main %>% ungroup() %>% group_by(category) %>% 
  mutate(responseCategory_count = n(),
         responseCategory_proportion = responseCategory_count / nrow(df_clean_main)
         ) %>% ungroup() %>%
  select(category, responseCategory_proportion) %>% unique() %>%
  mutate(model_name = "human") %>%
  rename(c("answer_type" = "category", "mean_prob" = "responseCategory_proportion"))

lm_scores_e2_summary <- lm_scores_e2_summary %>% 
  mutate(model_name = "GPT3-zeroShot") %>% select(-mean_ppl) %>%
  rbind(., e3_summary)

lm_scores_e2_summary %>% 
  mutate(model_name = factor(model_name, levels = c("human", "GPT3-zeroShot"))) %>%
  ggplot(., aes(x = answer_type, y = mean_prob, fill = answer_type, pattern = model_name)) +
  geom_col_pattern(alpha = 0.7, position = position_dodge(preserve = "single"),
                   width = bar.width,
                   color = "black", 
                   pattern_fill = "black",
                   pattern_angle = 45,
                   pattern_density = 0.1,
                   pattern_spacing = 0.025,
                   pattern_key_scale_factor = 0.6
                   ) +
  scale_pattern_manual(values = c(human = "stripe", `GPT3-zeroShot` = "none")) +
  labs(x = "Answer type", y = "Response type proportion", pattern = "Human data?") +
  guides(pattern = guide_legend(override.aes = list(fill = "white")),
         fill = guide_legend(override.aes = list(pattern = "none"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  theme(legend.title = element_text(size = 10), 
               legend.text = element_text(size = 10)) +
  ylab("Global proportion / probability\n of answer type") +
  xlab("Answer type") +
  ggtitle("Response type proportions of human data vs\nGPT3 zero shot probabilities")
```

```{r, echo=FALSE}
e2_gpt_samples <- read_csv("../code/results/experiment2/GPT3-davinci-003-samples-zeroShotLearner-m64-pilot4_categorized.csv")
e2_gpt_samples_vignette <- e2_gpt_samples %>%
  mutate(context_nr = case_when(
    itemName == 'sugar-coffee' ~ 'context1',
    itemName == 'sugar-baking' ~ 'context2',
    itemName == 'blanket-sleepover' ~ 'context1',
    itemName == 'blanket-transportation' ~ 'context2',
    itemName == 'chair-repair' ~ 'context1',
    itemName == 'chair-party' ~ 'context2',
    itemName == 'box-muffins' ~ 'context1',
    itemName == 'box-bbq' ~ 'context2',
    itemName == 'handkerchief-injury' ~ 'context1',
    itemName == 'handkerchief-sauce' ~ 'context2',
    itemName == 'broom-renovation' ~ 'context1',
    itemName == 'broom-snow' ~ 'context2',
    itemName == 'sweatpants-sleepover' ~ 'context1',
    itemName == 'sweatpants-spill' ~ 'context2',
    itemName == 'umbrella-summer' ~ 'context1',
    itemName == 'umbrella-rain' ~ 'context2',
    itemName == 'postit-board' ~ 'context1',
    itemName == 'postit-reading' ~ 'context2',
    itemName == 'pot-garden' ~ 'context1',
    itemName == 'pot-jam' ~ 'context2',
    itemName == 'towel-hair' ~ 'context1',
    itemName == 'towel-clothes' ~ 'context2',
    itemName == 'paper-bbq' ~ 'context1',
    itemName == 'paper-hamster' ~ 'context2',
    itemName == 'candy-halloween' ~ 'context1',
    itemName == 'candy-dessert' ~ 'context2',
    itemName == 'bottle-plants' ~ 'context1',
    itemName == 'bottle-water' ~ 'context2',
    TRUE ~ itemName
  ),
  category = factor(category, levels = c("competitor", "mostSimilar", "sameCategory", "otherCategory", "fullList", "taciturn")))%>% 
  rowwise() %>%
  mutate(
  settingName = str_split(itemName, "-", simplify=TRUE)[,1]
  )
e4_pilot_items_cats <- read_csv("../experiments/contextSensitive_free_production/trials/trials_split_cogsci_pilot4_ItemCategorization.csv") %>%
  select(-itemName, -itemQuestion)  
e2_gpt_samples_category <- e2_gpt_samples_vignette %>% filter((response_option != "yes") & (response_option != "no") & (response_option != "other")) %>%
  separate_rows(response_option, sep=",") %>%
  mutate(response_option = gsub(" ", "", response_option)) %>%
  left_join(., e4_pilot_items_cats, by = c("settingName", "response_option"))
  
e2_gpt_samples_category_wide <- e2_gpt_samples_category %>% group_by(settingName, context_nr, global_category) %>%
  mutate(responseCat_count = n()) %>% ungroup() %>% group_by(settingName, context_nr) %>%
  mutate(response_count = n()) %>% ungroup() %>%
  mutate(response_prop = responseCat_count / response_count
         ) %>% select(settingName, context_nr, category, global_category, response_prop) %>%
  unique() %>%
  pivot_wider(values_from = response_prop, names_from = context_nr) %>%
  mutate(context1 = ifelse(is.na(context1), 0, context1),
         context2 = ifelse(is.na(context2), 0, context2)
         ) %>% group_by(global_category) %>%
  mutate(mean_context1 = mean(context1),
         mean_context2 = mean(context2)) %>% select(-settingName, -context1, -context2, -category) %>% unique()

e2_gpt_samples_category_wide %>%
  ggplot(., aes(fill = global_category, x = global_category)) +
  geom_col(aes(y = mean_context1)) +
  geom_col(aes(y = -mean_context2)) +
 theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position = "right") +
  scale_y_continuous( limits = c(-0.6, 0.6)) +
  geom_hline(yintercept =0) +
  coord_flip() +
  ylab("context 2 <--> context 1") +
  xlab("Alternative category") +
  ggtitle("Response alternative mention proportions \nin GPT-3 zero shot samples")
```

```{r}
e2_gpt_samples_summary <- e2_gpt_samples_vignette %>% group_by(category) %>%
  filter(!(is.na(category))) %>%
  mutate(responseCat_count = n(),
         response_count = e2_gpt_samples_vignette %>% nrow(),
         prop = responseCat_count / response_count
         ) %>% select(category, prop) %>% unique() %>%
  mutate(category = factor(category, levels = c("competitor", "mostSimilar", "sameCategory", "fullList")))

e2_gpt_samples_summary %>%
  ggplot(., aes(x = category, y = prop, fill = category)) +
  geom_col() +
  labs(x = "Answer type", y = "Response type proportion") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  theme(legend.title = element_text(size = 10), 
               legend.text = element_text(size = 10)) +
  ylab("Global proportion of answer type") +
  xlab("Answer type") +
  ggtitle("Response type proportions of GPT3 zero shot probabilities")
```

## By prompt analysis
```{r}
oneShotQA_e1 <- read_csv("../code/results/prompts/oneShotExample/experiment1/GPT3-davinci-002-samples-oneShotLearner-n5-m64-cogsci-e1_categorized.csv") %>% mutate(
  prompt = "one-shot QA"
) %>% select(-probs) %>% filter(!is.na(category))
oneShotExample_e1 <- read_csv("../code/results/prompts/oneShotCoT_old_sanity-check/GPT3-davinci-002-samples-oneShotLearner-n5-m64-cogsci-e1_categorized.csv") %>% select(-probs) %>% mutate(prompt="one-shot CoT") %>% filter(!is.na(category))
#rename("prompt" = "model") %>% select(-`...1`, -probs)
#read_csv("../code/results/prompts/oneShotCoT/experiment1/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogsci_categorized.csv") %>% rename("prompt" = "model") %>% select(-`...1`, -probs)

#zeroShotExample_e1 <- read_csv("../code/results/categorized_responses/GPT3-davinci-003-samples-zeroShotLearner-m64-cogsci_categorized.csv") %>% select(-model_name, -probs) %>% mutate(prompt = "zero-shot") %>% filter(category != 'yes') %>% filter(!is.na(category))
zeroShotExample_e1 <- read_csv("../code/results/prompts/GPT3-davinci-002-samples-zeroShotLearner-m64-cogsci-e1_categorized.csv") %>% select(-probs) %>% mutate(prompt = "zero-shot") %>% filter(category != 'yes') %>% filter(!is.na(category))

oneShotExplanation_e1 <- read_csv("../code/results/prompts/oneShotExplanation/GPT3-davinci-002-samples-oneShotLearner-n5-m64-cogsci-e1_categorized.csv") %>% mutate(prompt = "one-shot Explanation") %>% select(-probs) %>% filter(!is.na(category))

#oneShotExplanation_e1_2 <- read_csv("../code/results/prompts/oneShotExplanation/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogsci-e1_categorized.csv") %>% mutate(prompt = "one-shot Explanation") %>% select(-probs) %>% filter(!is.na(category))


e1_humans <- read_csv("data/results_QA-overinfo-freeTyping-cogsci_full_anonymized_categorized_cleaned.csv") %>% select(category, answer, itemName) %>%
  mutate(prompt = "human E1") %>% rename("predictions" = "answer")
# oneShotExplanation_e1_2
prompts_e1 <- rbind(oneShotQA_e1, oneShotExample_e1, zeroShotExample_e1, oneShotExplanation_e1, e1_humans) %>% 
  mutate(prompt = factor(prompt, levels = c("zero-shot", "one-shot Explanation", "one-shot QA", "one-shot CoT", "human E1")))

prompts_e1_raw <- prompts_e1 %>% group_by(category, prompt) %>%
  mutate(category = ifelse(is.na(category), "taciturn", category))

prompts_e1_summary <- prompts_e1 %>% group_by(category, prompt) %>%
  mutate(category = ifelse(is.na(category), "taciturn", category),
    responseType_count = n()) %>% ungroup() %>% group_by(prompt) %>%
  mutate(prompt_count = n(),
         prop = responseType_count / prompt_count
         ) %>% select(category, prop, prompt) %>% unique()

prompts_e1_summary_labels <- prompts_e1_summary %>% group_by(category) %>%
  summarize(max_val = max(prop)) %>% 
  mutate(category = factor(category, levels = c("competitor", "sameCategory", "otherCategory", "fullList", "other", "taciturn"), labels = c("competitor\n(iced coffee)", "similar option\n(soda)", "unrelated\noption\n(Chardonnay)", "all options", "other", "no options") ))

prompts_e1_summary %>% 
  mutate(category = factor(category, levels = rev(c("taciturn", "other", "fullList", "otherCategory", "sameCategory", "competitor")), labels = rev(c("no options",  "other", "all options", "unrelated\noption\n(Chardonnay)", "similar option\n(soda)", "competitor\n(iced coffee)"  ) ))) %>%
  #mutate(category = factor(category, levels = rev(c("taciturn", "other", "fullList", "otherCategory", "sameCategory", "competitor")) )) %>%
  ggplot(., aes(x = category, y = prop, fill = prompt, label = category)) +
  geom_col(position = position_dodge(preserve = "single"), color = "#575463", width = 0.8 )+
  geom_text(inherit.aes = FALSE, data = prompts_e1_summary_labels, aes(x = category, y = max_val, label = category), stat = 'unique', nudge_y = 0.08, size = 4, check_overlap = TRUE) +
  theme_csp() +
  scale_y_continuous( limits = c(0, 0.7), breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7)) +
   theme(plot.title = element_text(hjust = 0.5), legend.position = "right",  legend.title = element_text(size=12), legend.text = element_text(size=12), axis.text.x = element_blank(), axis.ticks.x = element_blank() ) +  # axis.text.x = element_text(angle = 45, hjust = 1), 
  #coord_flip() +
  ylab("Proportion of answer type") +
  xlab("Answer type") +
  ggtitle("davinci-002: Do you have iced tea?")

#ggsave("viz/e1_human_gpt3_lessNA_explAvg_horizontal.pdf", width = 7.5, height = 4 )
```

```{r}
oneShotQA_e2 <- read_csv("../code/results/prompts/oneShotExample/experiment2/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogci_categorized.csv") %>% mutate(
  prompt = "one-shot QA"
) %>% select(-response_option )
oneShotExample_e2 <- read_csv("../code/results/prompts/oneShotCoT/experiment2/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogsci_categorized.csv") %>% mutate(prompt = "one-shot CoT") #%>% select(-`...1`)

zeroShotExample_e2 <- read_csv("../code/results/experiment2/GPT3-davinci-003-samples-zeroShotLearner-m64-pilot4_categorized.csv") %>% select(-`...1`, -response_option) %>% 
  mutate(prompt = "zero-shot") %>% filter(category != 'yes')

prompts_e2 <- rbind(oneShotQA_e2, oneShotExample_e2, zeroShotExample_e2) %>% 
  mutate(prompt = factor(prompt, levels = c("zero-shot", "one-shot QA", "one-shot CoT")))

prompts_e2_summary <- prompts_e2 %>% group_by(category, prompt) %>%
  mutate(category = ifelse(is.na(category), "none", category),
    responseType_count = n()) %>% ungroup() %>% group_by(prompt) %>%
  mutate(prompt_count = n(),
         prop = responseType_count / prompt_count
         ) %>% select(category, prop, prompt) %>% unique() %>% ungroup() %>%
  add_row(category="otherCategory", prop=0, prompt="zero-shot") %>%
  add_row(category="otherCategory", prop=0, prompt="one-shot QA")%>%
add_row(category="otherCategory", prop=0, prompt="one-shot CoT")

prompts_e2_summary %>% 
  mutate(category = factor(category, levels = c('other', 'taciturn', 'fullList',  'otherCategory', 'sameCategory', 'mostSimilar','competitor'),
                           labels = c("other", "no option", "all options", "unrelated option\n(broom)","similar option\n(stool)", "related option\n(recliner)", "competitor\n(ladder)")
                           )) %>%
  ggplot(aes(x = category, y = prop, fill = prompt, label = category)) +
  geom_col(position = position_dodge(preserve = "single"), color = "#575463" )+
  #geom_text(inherit.aes = FALSE, data = prompts_e1_summary_labels, aes(x = category, y = max_val, label = category), stat = 'unique', nudge_y = 0.05, size = 3.5, check_overlap = TRUE) +
  theme_csp() +
  scale_y_continuous( limits = c(0, 0.6), breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6)) +
   theme(plot.title = element_text(hjust = 0.5), legend.position = "right",  legend.title = element_text(size=12), legend.text = element_text(size=12), axis.text.y = element_blank(), axis.ticks.y =  element_blank()) +  # axis.text.x = element_text(angle = 45, hjust = 1), 
  coord_flip() +
  ylab("") +
  xlab("")

ggsave("viz/e2_byPrompt_vertical_final_LR.pdf", width = 5.5, height = 4 )
```

```{r}
e4_pilot_items_cats <- read_csv("../experiments/contextSensitive_free_production/trials/trials_split_cogsci_pilot4_ItemCategorization.csv") %>%
  select(-itemName, -itemQuestion)  
e4_pilot_vignettes <- read_csv("../experiments/contextSensitive_free_production/trials/trials_split_cogsci_pilot4.csv") %>% 
  select(itemName, settingName) %>%
  mutate(context_nr = rep(c("context1", "context2"), 13))


oneShotQA_e2 <- read_csv("../code/results/prompts/oneShotExample/experiment2/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogci_categorized.csv") %>% mutate(
  response_option = ifelse(is.na(response_option), "taciturn", response_option),
  prompt = "one-shot QA",
  settingName = str_split(itemName, "-", simplify=TRUE)[,1]
) %>% filter((response_option != "yes") & (response_option != "no") & (response_option != "other") & (response_option != "taciturn")) %>%
  left_join(., e4_pilot_vignettes, by = c("itemName", "settingName")) %>%
  group_by(itemName) %>%
  mutate(vignette_count = n()) %>% ungroup() %>%
  separate_rows(response_option, sep=",") %>%
  mutate(response_option = gsub(" ", "", response_option)) %>%
  left_join(., e4_pilot_items_cats, by = c("settingName", "response_option"))  %>% ungroup() %>% group_by(global_category, itemName) %>%
  mutate(responseCategory_count = n()) %>% ungroup() %>% 
  mutate(subjMention_prop = responseCategory_count / vignette_count) %>% group_by(global_category, prompt, context_nr) %>%
  summarize(subjMention_mean = mean(subjMention_prop)) %>% 
  pivot_wider(values_from = subjMention_mean, names_from = context_nr)

oneShotExample_e2 <- read_csv("../code/results/prompts/oneShotCoT_old_sanity-check/experiment2/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogsci-e2_categorized.csv") %>% #read_csv("../code/results/prompts/oneShotCoT/experiment2/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogsci_categorized.csv") %>% 
  mutate(
  response_option = ifelse(is.na(response_option), "taciturn", response_option),
  prompt = "one-shot CoT",
      settingName = str_split(itemName, "-", simplify=TRUE)[,1]
      )  %>% filter((response_option != "yes") & (response_option != "no") & (response_option != "other") & (response_option != "taciturn")) %>%
  left_join(., e4_pilot_vignettes, by = c("itemName", "settingName")) %>%
  group_by(itemName) %>%
  mutate(vignette_count = n()) %>% ungroup() %>%
  separate_rows(response_option, sep=",") %>%
  mutate(response_option = gsub(" ", "", response_option)) %>%
  left_join(., e4_pilot_items_cats, by = c("settingName", "response_option")) %>% ungroup() %>% group_by(global_category, itemName) %>%
  mutate(responseCategory_count = n()) %>% ungroup() %>% 
  mutate(subjMention_prop = responseCategory_count / vignette_count)  %>% group_by(global_category, prompt, context_nr) %>%
  summarize(subjMention_mean = mean(subjMention_prop))  %>% 
  pivot_wider(values_from = subjMention_mean, names_from = context_nr)

oneShotExplanation_e2 <- read_csv("../code/results/prompts/oneShotExplanation/experiment2/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogsci-e2_categorized.csv") %>% #read_csv("../code/results/prompts/oneShotCoT/experiment2/GPT3-davinci-003-samples-oneShotLearner-n5-m64-cogsci_categorized.csv") %>% 
  mutate(
  response_option = ifelse(is.na(response_option), "taciturn", response_option),
  prompt = "one-shot Explanation",
      settingName = str_split(itemName, "-", simplify=TRUE)[,1]
      )  %>% filter((response_option != "yes") & (response_option != "no") & (response_option != "other") & (response_option != "taciturn")) %>%
  left_join(., e4_pilot_vignettes, by = c("itemName", "settingName")) %>%
  group_by(itemName) %>%
  mutate(vignette_count = n()) %>% ungroup() %>%
  separate_rows(response_option, sep=",") %>%
  mutate(response_option = gsub(" ", "", response_option)) %>%
  left_join(., e4_pilot_items_cats, by = c("settingName", "response_option")) %>% ungroup() %>% group_by(global_category, itemName) %>%
  mutate(responseCategory_count = n()) %>% ungroup() %>% 
  mutate(subjMention_prop = responseCategory_count / vignette_count)  %>% group_by(global_category, prompt, context_nr) %>%
  summarize(subjMention_mean = mean(subjMention_prop))  %>% 
  pivot_wider(values_from = subjMention_mean, names_from = context_nr)

zeroShotExample_e2 <- read_csv("../code/results/experiment2/GPT3-davinci-003-samples-zeroShotLearner-m64-pilot4_categorized.csv") %>% select(-`...1`) %>% 
  mutate(response_option = ifelse(is.na(response_option), "taciturn", response_option),
    prompt = "zero-shot",
         settingName = str_split(itemName, "-", simplify=TRUE)[,1]) %>% 
  filter((response_option != "yes") & (response_option != "no") & (response_option != "other")) %>%
  left_join(., e4_pilot_vignettes, by = c("itemName", "settingName")) %>%
  group_by(itemName) %>%
  mutate(vignette_count = n()) %>% ungroup() %>%
  separate_rows(response_option, sep=",") %>%
  mutate(response_option = gsub(" ", "", response_option)) %>%
  left_join(., e4_pilot_items_cats, by = c("settingName", "response_option"))  %>% ungroup() %>% group_by(global_category, itemName) %>%
  mutate(responseCategory_count = n()) %>% ungroup() %>% 
  mutate(subjMention_prop = responseCategory_count / vignette_count) %>% group_by(global_category, prompt, context_nr) %>%
  summarize(subjMention_mean = mean(subjMention_prop)) %>% 
  pivot_wider(values_from = subjMention_mean, names_from = context_nr)

human_e2 <- read_csv("data/results_105_QA-overinfo-contextDependent-freeTyping-pilot4_130_byContext_summary.csv") %>% 
  mutate(prompt = "human E2")

prompts_e2 <- rbind(oneShotQA_e2, oneShotExample_e2, oneShotExplanation_e2,  zeroShotExample_e2, human_e2) %>% 
  mutate(prompt = factor(prompt, levels = c("zero-shot", "one-shot Explanation", "one-shot QA", "one-shot CoT", "human E2")))

prompts_e2 %>% 
  mutate(global_category = factor(global_category, levels = c( 'otherCategory', 'mostSimilar', 'competitor_c2', 'competitor_c1'),
                           labels = c("unrelated option\n(broom)", "a priori\nsimilar (recliner)", "competitor 2\n(stool)", "competitor 1\n(ladder)")
                           )) %>%
  ggplot(., aes(fill = prompt, x = global_category)) +
  geom_col(aes(y = context1), position = position_dodge(preserve = "single"), color = "#575463" ) +
  geom_col(aes(y = -context2), position = position_dodge(preserve = "single"), color = "#575463" ) +
  theme_csp() + 
  theme( axis.text.y = element_text(size = 12), axis.text.x = element_text(size = 12),  legend.title = element_text(size=12), legend.text = element_text(size=12), legend.position = "right", plot.title = element_text(hjust = 0.5)) + # , axis.ticks.y = element_blank(), 
  scale_y_continuous( limits = c(-1, 1), breaks = c(-1, -0.5, 0, 0.5, 1), labels = c("1", "0.5", "0", "0.5", "1")) +
  geom_hline(yintercept = 0, color = "white", size = 1) +
  coord_flip() +
  ylab("party <--> reaching window top") +
  xlab("") +
  ggtitle("Do you have a chair?")


#ggsave("viz/e2_byPrompt_vertical_final_lessNA.pdf", width = 5.5, height = 4 )
```

```{r}
# df for e1 models vs humans
e1_raw_models_vs_humans <- nm_samples_categorized %>% filter(model_name != "gpt3-davinci-003") %>% select(model_name, category) %>% rbind(., prompts_e1_raw %>% rename( "model_name" = "prompt") %>% select(model_name, category)) %>% ungroup()
e1_raw_models_vs_humans_labels <- e1_raw_models_vs_humans %>% distinct(model_name) %>% .$model_name

e1_raw_models_vs_humans <- e1_raw_models_vs_humans %>% mutate(
  model_name = factor(model_name, levels = rev(e1_raw_models_vs_humans_labels))
)
# model for e 1
e1_human_vs_models <- brm(category ~ 1 + model_name,
                       data = e1_raw_models_vs_humans,
                       family = "categorical",
                       iter = 4000,
                       cores = 4
                       )

summary(e1_human_vs_models)

e1_humans %>% group_by(category) %>% filter(category!="otherCategory") %>% mutate(category = factor(category, levels = c("competitor", "sameCategory", "fullList", "taciturn", "other"))) %>% summarize(count = n())
prompts_e1_summary %>% ungroup() %>% mutate(category = factor(category, levels = c("competitor", "sameCategory", "fullList", "taciturn", "other"))) %>% filter(prompt == "one-shot CoT")  -> e1_p
e1_pval <- e1_humans %>% group_by(category) %>% filter(category!="otherCategory") %>% mutate(category = factor(category, levels = c("competitor", "sameCategory", "fullList", "taciturn", "other"))) %>% summarize(count = n()) %>% left_join(., e1_p, by="category")

chisq.test(x = e1_pval %>% pull(count), p = e1_pval %>% pull(prop), simulate.p.value = TRUE)
```

