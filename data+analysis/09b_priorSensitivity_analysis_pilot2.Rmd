---
title: 'PragmaticQA E4: Prior Sensitivity Free Production: Pilot 2'
author: "PT"
date: "2024-04-30"
output: github_document
---

```{r, include=FALSE}
library(tidyverse)
library(tidyboot)
library(aida)
library(brms)
library(tidybayes)
library(cspplot)
```

```{r, include=FALSE}
# these options help Stan run faster
options(mc.cores = parallel::detectCores())
```

# Intro

This is another experiment in the PragmaticQA project, wherein we investigate whether speakers' overinformativeness in response to polar questions depends on the (commonly known) prior probability of the available options. The live experiment can be found [here](https://magpie-ea.github.io/magpie3-qa-overinfo-free-production/experiments/04-priorSensitivity_free_production/). We manipulate the prior probability of the available options in context, resulting in two critical conditions, highPrior (high prior options available) and lowPrior (high prior option not available).
The experiment is a free production experiment. Each participant sees four main trials (two per condition, assigned at random to four randomly chosen vignettes), and one filler trial. Fillers are re-used from CogSci E1.

* Pilot 1: five vignettes only were chosen here: cardBlanche, icecreamFlavors, streamingOptions, coffeeShops, socialMedia. N=30 were recruited.
* **Pilot 2** (analysed here): we use ten vignettes after the logical structure was adjusted. All items were used. N=30 were recruited.

```{r, include=FALSE, message=FALSE}
d <- read_csv("data/results_PragmaticQA-E3-priorSensitivity-pilot-2.csv")
# number of recruited subjects
cat("Number of recruited subjects: ", d %>% pull(submission_id) %>% unique() %>% length())

# attention check based exclusions
# check attention check completion
df_attention <- d %>% filter(trial_type == "filler") %>% rowwise() %>%
  mutate(passed_trial = grepl(tolower(correct_response), tolower(answer))) 

df_attention_fail <- df_attention %>% group_by(submission_id) %>% 
  mutate(passed_subj = sum(passed_trial) > 0)

# participants who failed attention checks 
subj_id_attention_fails <- df_attention_fail %>% filter(passed_subj == FALSE) %>% pull(submission_id) %>% unique()
cat("Subjects who failed attention checks: ", subj_id_attention_fails)  
cat("Number of subjects who failed attention checks: ", length(subj_id_attention_fails))

# filtering out main trials only and only subjects who passed attention checks
d_main <- d %>% filter(!(submission_id %in% subj_id_attention_fails)) %>% 
  filter(condition != "filler")
```

We exclude the following invalid response categories:

* "no": participants answered no to the question (although all questions were yes-questions)
* "other": uncategorizable responses
* "yes": all three options were stated as available

Furthermore, some responses had two categories, namely the full list (i.e., the two available options were named) and the exceptive phrase (i.e, it was stated that one option was not available). Such responses were split into two data points with single labels.

```{r}
d_main_clean <- d_main %>%
  filter(category != "no") %>%
  filter(category != "other") %>%
  filter(category != "yes") %>%
  mutate(
    category = str_split(category, ", ", simplify = FALSE) 
  ) %>%
  unnest(category)
  
d_main_clean
```

The freely typed responses were manually categorized into the following categories:

* "taciturn": responses just saying "yes" or "yes, we do" or similar.
* "taciturn_more": responses saying "yes" but providing some additional information without mentioning specific options, for instance "Yes there are some coffee shops around the corner"
* "mostLikely": response mentioning the most likely option among the available ones, e.g., "Yes we do accept American Express"
* "leastLikely": only one option was named, namely the least likely one among the available ones, e.g., "Yes we do accept Visa" in the high prior condition
* "fullList": responses mentioning the two specific available options
* "exceptive: responses mentioning which option is NOT available, e.g., "Yes, all except Card Blanche"

Below the single category proportions are displayed by condition.
```{r}
df_answerOptions_global_summary <- d_main_clean %>% 
  group_by(category) %>% 
  summarise(answerType_count = n(), 
            answerType_proportion = answerType_count / nrow(d)
            )

df_answerOptions_byCondition_summary <- d_main_clean %>% 
  group_by(condition) %>%
  mutate(condition_counts = n()) %>%
  group_by(category, condition) %>% 
  summarise(answerType_count = n(), 
            answerType_proportion = answerType_count / condition_counts
            ) %>% unique()


df_answerOptions_byCondition_summary %>%
  ggplot(aes(x = condition, y = answerType_proportion, fill = category)) +
  geom_col(color = "#575463") +
  theme_csp() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Proportion of answer type") +
  xlab("") 
```

Below, the response categories are plotted by-item, so as to check if respondents might have chosen different strategies, e.g., full-list responses, in e.g. commercial contexts so as to not loose customers:
```{r, fig.height = 12}
df_answerOptions_byCondition_byItem_summary <- d_main_clean %>% 
  group_by(itemName, condition) %>%
  mutate(condition_counts = n()) %>%
  group_by(itemName, category, condition) %>% 
  summarise(answerType_count = n(), 
            answerType_proportion = answerType_count / condition_counts
            ) %>% unique()


df_answerOptions_byCondition_byItem_summary %>%
  ggplot(aes(x = condition, y = answerType_count, fill = category)) +
  geom_col(color = "#575463") +
  theme_csp() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Proportion of answer type") +
  facet_wrap(~itemName, ncol=3) +
  xlab("") 
```

We can also looks at the bootstrapped CIs over the rate of exceptive answers; while the difference is larger, the CIs overlap.
```{r}
d_main_binary_summary <- d_main_clean %>% 
  mutate(is_exceptive = ifelse(category == "exceptive", 1, 0)) %>%
  group_by(condition) %>% 
  tidyboot_mean(column = is_exceptive)
d_main_binary_summary
```
