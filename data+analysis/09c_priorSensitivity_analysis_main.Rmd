---
title: 'PragmaticQA E4: Prior Sensitivity Free Production: Main'
author: "PT"
date: "2024-04-30"
output: github_document
---

```{r, include=FALSE}
library(tidyverse)
library(tidyboot)
library(aida)
library(brms)
library(tidybayes)
library(cspplot)
```

```{r, include=FALSE}
# these options help Stan run faster
options(mc.cores = parallel::detectCores())
```

# Intro

This is another experiment in the PragmaticQA project, wherein we investigate whether speakers' overinformativeness in response to polar questions depends on the (commonly known) prior probability of the available options. The live experiment can be found [here](https://magpie-ea.github.io/magpie3-qa-overinfo-free-production/experiments/04-priorSensitivity_free_production/). We manipulate the prior probability of the available options in context, resulting in two critical conditions, highPrior (high prior options available) and lowPrior (high prior option not available).
The experiment is a free production experiment. Each participant sees four main trials (two per condition, assigned at random to four randomly chosen vignettes), and one filler trial. Fillers are re-used from CogSci E1.

* Main experiment: N=99 were recruited. Ten items were used (as in pilot 2). Therefore, the combined data of pilot 2 (N=30) and the main data collection are analysed here.

```{r, include=FALSE, message=FALSE}
d_pilot <- read_csv("data/results_PragmaticQA-E3-priorSensitivity-pilot-2.csv")
d_main <- read_csv("data/results_PragmaticQA-E3-priorSensitivity-main.csv")
d <- rbind(d_pilot, d_main)
# number of recruited subjects
cat("Number of recruited subjects: ", d %>% pull(submission_id) %>% unique() %>% length())

# attention check based exclusions
# check attention check completion
df_attention <- d %>% filter(trial_type == "filler") %>% rowwise() %>%
  mutate(passed_trial = grepl(tolower(correct_response), tolower(answer))) 

df_attention_fail <- df_attention %>% group_by(submission_id) %>% 
  mutate(passed_subj = sum(passed_trial) > 0)

# participants who failed attention checks 
subj_id_attention_fails <- df_attention_fail %>% filter(passed_subj == FALSE) %>% pull(submission_id) %>% unique()
cat("Subjects who failed attention checks: ", subj_id_attention_fails)  
cat("Number of subjects who failed attention checks: ", length(subj_id_attention_fails))

# filtering out main trials only and only subjects who passed attention checks
d_main <- d %>% filter(!(submission_id %in% subj_id_attention_fails)) %>% 
  filter(condition != "filler")

cat("Number of analysed subjects who passed attention checks: ", d_main %>% pull(submission_id) %>% unique() %>% length())
```

We exclude the following invalid response categories:

* "no": participants answered no to the question (although all questions were yes-questions)
* "other": uncategorizable responses
* "yes": all three options were stated as available

Furthermore, some responses had two categories, namely the full list (i.e., the two available options were named) and the exceptive phrase (i.e, it was stated that one option was not available). Such responses were split into two data points with single labels.

```{r}
d_main_clean <- d_main %>%
  filter(category != "no") %>%
  filter(category != "other") %>%
  filter(category != "yes") %>%
  mutate(
    category = str_split(category, ", ", simplify = FALSE) 
  ) %>%
  unnest(category) %>%
  mutate(
    category = ifelse(category == "taciturn_more", "taciturn", category)
  )
  
d_main_clean
```

The freely typed responses were manually categorized into the following categories:

* "taciturn": responses just saying "yes" or "yes, we do" or similar.
* "taciturn_more": responses saying "yes" but providing some additional information without mentioning specific options, for instance "Yes there are some coffee shops around the corner"
* "mostLikely": response mentioning the most likely option among the available ones, e.g., "Yes we do accept American Express"
* "leastLikely": only one option was named, namely the least likely one among the available ones, e.g., "Yes we do accept Visa" in the high prior condition
* "fullList": responses mentioning the two specific available options
* "exceptive: responses mentioning which option is NOT available, e.g., "Yes, all except Card Blanche"

Below the single category proportions are displayed by condition. For the purposes of the main analysis, taciturn and taciturn_more responses are collapsed.

```{r}
df_answerOptions_global_summary <- d_main_clean %>% 
  group_by(category) %>% 
  summarise(answerType_count = n(), 
            answerType_proportion = answerType_count / nrow(d)
            )

df_answerOptions_byCondition_summary <- d_main_clean %>% 
  filter(category != "no") %>%
  group_by(condition) %>%
  mutate(condition_counts = n()) %>%
  group_by(category, condition) %>% 
  summarise(answerType_count = n(), 
            answerType_proportion = answerType_count / condition_counts
            ) %>% unique()


df_answerOptions_byCondition_summary %>%
  ggplot(aes(x = condition, y = answerType_proportion, fill = category)) +
  geom_col(color = "#575463") +
  theme_csp() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Proportion of answer type") +
  xlab("") 
```

Below, the response categories are plotted by-item, so as to check if respondents might have chosen different strategies, e.g., full-list responses, in e.g. commercial contexts so as to not loose customers:
```{r, fig.height = 12}
df_answerOptions_byCondition_byItem_summary <- d_main_clean %>% 
  filter(category != "no") %>%
  group_by(itemName, condition) %>%
  mutate(condition_counts = n()) %>%
  group_by(itemName, category, condition) %>% 
  summarise(answerType_count = n(), 
            answerType_proportion = answerType_count / condition_counts
            ) %>% unique()


df_answerOptions_byCondition_byItem_summary %>%
  ggplot(aes(x = condition, y = answerType_count, fill = category)) +
  geom_col(color = "#575463") +
  theme_csp() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Proportion of answer type") +
  facet_wrap(~itemName, ncol=3) +
  xlab("") 
```

We can also check whether there is a credible difference between the rate of fullList responses (i.e., both fullList and exceptive responses) in the two conditions, by fitting a Bayesian logistic regression with a main effect of condition and maximal random effects. Uninformative priors are used. The hypothesis is that there are more fullList responses in the lowPrior condition than in the highPrior condition. 

```{r, warning=FALSE, message=FALSE}
d_main_fullList_binary <- d_main_clean %>% 
  mutate(is_fullList = ifelse(category == "exceptive", 1, ifelse(category == "fullList", 1, 0))) 

priors <- set_prior("student_t(1, 0, 0.25)", class = "b")

lm_full <- brm(
  is_fullList ~ condition + (1 + condition | submission_id) + (1 + condition | itemName),
  data = d_main_fullList_binary,
  family = "bernoulli",
  prior = priors,
  control = list(adapt_delta = 0.95),
  iter = 3000,
  chains = 4
)
```

```{r}
summary(lm_full)
```
The same analysis is done with the rate of "exceptive" answers only. The same hypothesis is tested (i.e., more exceptive answers are expected in the lowPrior than in the highPrior condition).
```{r, warning=FALSE, message=FALSE}
d_main_exceptive_binary <- d_main_clean %>% 
  mutate(is_exceptive = ifelse(category == "exceptive", 1, 0)) 

priors <- set_prior("student_t(1, 0, 0.25)", class = "b")

lm_exceptive <- brm(
  is_exceptive ~ condition + (1 + condition | submission_id) + (1 + condition | itemName),
  data = d_main_exceptive_binary,
  family = "bernoulli",
  prior = priors,
  control = list(adapt_delta = 0.95),
  iter = 3000,
  chains = 4
)
```

```{r}
summary(lm_exceptive)
```

We also check whether there is a credible difference between the rate of taciturn responses in the two conditions, by fitting a Bayesian logistic regression with a main effect of condition and maximal random effects. We expect more taciturn responses in the highPrior condition compared to the lowPrior condition.
```{r, warning=FALSE, message=FALSE}
d_main_taciturn_binary <- d_main_clean %>% 
  mutate(is_taciturn = ifelse(category == "taciturn", 1, 0)) 

lm_taciturn <- brm(
  is_taciturn ~ condition + (1 + condition | submission_id) + (1 + condition | itemName),
  data = d_main_taciturn_binary,
  family = "bernoulli",
  prior = priors,
  control = list(adapt_delta = 0.95),
  iter = 3000,
  chains = 4
)
```

```{r}
summary(lm_taciturn)
```