---
title: "QA prior elicitation & free typing analysis"
author: "Polina Tsvilodub"
date: '2022-11-06'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(tidyboot)
library(aida)
library(forcats)
library("gridExtra")
```

```{r, include=FALSE}
# these options help Stan run faster
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values = project_colors)
} 
```

## Intro

Below, exploratory analysis of the prior elicitation QA experiment data can be found. In the end, the results are compared against the free production results. Details of the free production analysis can be found [here](https://github.com/magpie-ea/magpie3-qa-overinfo-free-production/blob/main/data%2Banalysis/01_main_free_typing_analysis.md).

```{r, include=FALSE, warning=FALSE, message=FALSE}
answerOrder <- c( 'competitor', 'sameCategory1', 'sameCategory2', 'otherCategory1', 'otherCategory2')
#df <- read_csv("../../raw_data/results_97_QA-overinformative-priorElicitation-magpie_80.csv") 
#df %>% select(-prolific_pid, -prolific_session_id, -prolific_study_id) %>% write_csv("results_QA-overinfo-priorElicitation-magpie-80_anonymized.csv")

df <- read_csv("results_QA-overinfo-priorElicitation-magpie-80_anonymized.csv")
head(df)
cat("Number of recruited subjects: ", df %>% pull(submission_id) %>% unique() %>% length())
```

Participants failing all attention checks (3 out of 11 trials) are excluded from analysis. The attention checks consisted of trials where participants read instructions to move all sliders all the way to the left or to the right.

```{r, echo=FALSE}
# check attention check completion
df <- df %>% mutate(
  expected_attention = case_when(
    itemName == 'jobCenter-office' ~ 0,
    itemName == 'jobCenter-engineer' ~ 0,
    itemName == 'art-painting' ~ 100,
    itemName == 'art-drawing' ~ 100,
    itemName == 'carRental-fun' ~ 0,
    itemName == 'carRental-moving' ~ 0,
    itemName == 'music-hardrock' ~ 100,
    itemName == 'music-softrock' ~ 100,
    itemName == 'airport-usa' ~ 0,
    itemName == 'airport-europe' ~ 100,
    TRUE ~ 50
  )
)

df_attention <- df %>% filter(trial_type == "filler") %>% 
  select(submission_id, competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2, expected_attention, itemName) %>% 
  pivot_longer(cols = c(competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2), names_to = "answerType", values_to = "response") %>%
  rowwise() %>%
  mutate(passed_trial = abs(response - expected_attention) < 5 )

df_attention_fail <- df_attention %>% group_by(submission_id) %>% 
  mutate(passed_subj = sum(passed_trial) > 0)

df_attention_fail %>% ungroup() %>% filter( passed_subj == FALSE) %>% count(itemName, passed_subj)

# participants who failed attention checks 
subj_id_attention_fails <- df_attention_fail %>% filter(passed_subj == FALSE) %>% pull(submission_id) %>% unique()
cat("Numbrer of subjects who failed attention checks: ", length(subj_id_attention_fails)  )
cat("\nSubject exclusion rate: ", length(subj_id_attention_fails)/(unique(df$submission_id) %>% length()))
```

To understand what is going on in the attention checks, plot the ratings in the attention checking trials only, selecting participants who failed. It seems that many participants simply ignored the instructions and followed "anticipated" ratings.
```{r, fig.width = 8, fig.height = 12}
df_attention %>%
  filter(submission_id %in% subj_id_attention_fails) %>%
  mutate(answerType = factor(answerType, levels=answerOrder)) %>%
  group_by(itemName, answerType) %>%
  summarize(mean_response = mean(response)) %>%
  ggplot(aes(x = answerType, fill = answerType, y = mean_response)) +
  geom_col() +
  facet_wrap(itemName~., ncol=4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Mean ratings") +
  xlab("Alternative type")
```

```{r, echo=FALSE}
# get main clean data and center responses
df_clean_main <- df %>% filter(!(submission_id %in% subj_id_attention_fails)) %>%
  filter(trial_type == "main") 

df_clean_main_long <- df_clean_main %>%
  select(itemName, submission_id, competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2) %>%
   pivot_longer(cols = c(competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2), names_to = 'answerType', values_to = "response") %>%
  mutate(
    centered_response = response - 50
  )
```

We further exclude participants who provide the same responses on all trials (i.e. responses within the range of 5 points, basically just click trough the experiment).

```{r, echo=FALSE}
df_bad_subj <- df_clean_main_long %>% group_by(submission_id) %>%
  mutate(bad_subj = (max(response) - min(response)) < 5)
df_bad_subj %>% filter(bad_subj == TRUE)
cat("\nnumber of subjects who provided the same responses within 5 points on all main trials:",  df_bad_subj %>% filter(bad_subj == TRUE) %>% pull(submission_id) %>% unique() %>% length())
bad_subj_ids <- df_bad_subj %>% filter(bad_subj == TRUE) %>% pull(submission_id) %>% unique()

df_clean_main <- df_clean_main %>% filter(!(submission_id %in% bad_subj_ids))
df_clean_main_long <- df_clean_main_long %>% filter(!(submission_id %in% bad_subj_ids))

```
Characteristics of the analysed clean dataset:
```{r, echo=FALSE}
cat("\nNumber of analysed vignette responses: ", nrow(df_clean_main))

df_clean_main %>% count(itemName) 

cat("\naverage number of responses per vignette:", mean(df_clean_main %>% count(itemName) %>% pull(n)))

cat("\nvignette with most responses: ", df_clean_main %>% count(itemName) %>% arrange(desc(n)) %>% .[1,] %>% .$itemName, df_clean_main %>% count(itemName) %>% arrange(desc(n)) %>% .[1,] %>% .$n)
cat("\nvignette with least responses: ", df_clean_main %>% count(itemName) %>% arrange(n) %>% .[1,] %>% .$itemName, df_clean_main %>% count(itemName) %>% arrange(n) %>% .[1,] %>% .$n)

```


```{r, include=FALSE, message=FALSE, warning=FALSE}
# wrangle target names and answer options for easier parsing
df_vignettes <- read_csv("../experiments/prior_elicitation/trials/trials_split.csv") %>% 
  select(itemName, itemQuestion, competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2)

df_clean_main_wItems <- df_clean_main %>% left_join(., df_vignettes, by="itemName", suffix=c("", "_string"))

df_clean_main_wItems_woExclusions <- df %>% filter(trial_type == "main") %>%
  left_join(., df_vignettes, by="itemName", suffix=c("", "_string"))

df_clean_main_wItems_long <- df_clean_main_wItems %>%
  select(itemName, submission_id, competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2, itemQuestion, competitor_string, sameCategory1_string, sameCategory2_string, otherCategory1_string, otherCategory2_string) %>%
   pivot_longer(cols = c(competitor_string, sameCategory1_string, sameCategory2_string, otherCategory1_string, otherCategory2_string), names_to = 'answerType_string', values_to = "answerOption_string") %>% 
  mutate(answerType_string = gsub("_string", "", answerType_string)) %>%
  pivot_longer(cols = c(competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2), names_to = 'answerType', values_to = "response") %>% 
  filter(answerType_string == answerType) %>%
  mutate(answerType = factor(answerType, levels = answerOrder),
         centered_response = response - 50)

df_clean_main_wItems_woExclusions_long <- df_clean_main_wItems_woExclusions %>%  
  select(itemName, submission_id, competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2, itemQuestion, competitor_string, sameCategory1_string, sameCategory2_string, otherCategory1_string, otherCategory2_string) %>%
   pivot_longer(cols = c(competitor_string, sameCategory1_string, sameCategory2_string, otherCategory1_string, otherCategory2_string), names_to = 'answerType_string', values_to = "answerOption_string") %>% 
  mutate(answerType_string = gsub("_string", "", answerType_string)) %>%
  pivot_longer(cols = c(competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2), names_to = 'answerType', values_to = "response") %>% 
  filter(answerType_string == answerType) %>%
  mutate(answerType = factor(answerType, levels = answerOrder),
         centered_response = response - 50)

```

The first plot below shows the raw prior ratings (y-axis) against the alternative category (i.e., competitor, sameCategory1, otherCategory1 etc; x-axis). The second plot shows only by-vignette by-alternative average ratings across participants. The horizontal dashed line represents no change in beliefs of the participants about the alternative, given the context. The error bars represent 95% bootstrapped credible intervals.

```{r, echo=FALSE, fig.height=28, fig.width=12}
# plot by vignette
df_clean_main_centered <- df_clean_main %>%
  mutate(competitor = competitor - 50,
         sameCategory1 = sameCategory1 - 50,
         sameCategory2 = sameCategory2 - 50,
         otherCategory1 = otherCategory1 - 50,
         otherCategory2 = otherCategory2 - 50) 
  

df_clean_main_summary_unique <- df_clean_main_centered %>%
  select(itemName, competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2) %>%
   pivot_longer(cols = -itemName, names_to = 'answerType', values_to = "count") %>%
  group_by(itemName, answerType) %>%
  tidyboot_mean(column = count)



df_clean_main_long  %>%
  mutate(answerType = factor(answerType, levels = c('competitor', 'sameCategory1', 'sameCategory2', 'otherCategory1', 'otherCategory2'))) %>%
  ggplot(aes(x = answerType, color = answerType, y = centered_response)) +
  geom_point() +
  geom_hline(yintercept=0, linetype='dashed') +
  facet_wrap( itemName ~ . , ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Raw ratings") +
  ylim(-50, 50) +
  xlab("Alternative type")

df_clean_main_summary_unique  %>%
  mutate(answerType = factor(answerType, levels = c('competitor', 'sameCategory1', 'sameCategory2', 'otherCategory1', 'otherCategory2'))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = mean)) +
  geom_col() +
  geom_hline(yintercept=0, linetype='dashed') +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width = 0.2) +
  facet_wrap( itemName ~ . , ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Average rating") +
  ylim(-50, 50) +
  xlab("Alternative type")

#ggsave("viz/priorElicitation_byAnswerOption_80_wErrorBars.pdf", width = 12, height = 28)
```

The global plot below shows by-category ratings averaging over vignettes. The error bars represent 95% bootstrapped credible intervals.
```{r, echo=FALSE, fig.height=6, fig.width=10}
df_clean_main_centered %>%
  select(itemName, competitor, sameCategory1, sameCategory2, otherCategory1, otherCategory2) %>%
   pivot_longer(cols = -itemName, names_to = 'answerType', values_to = "count") %>%
  group_by(answerType) %>%
  tidyboot_mean(column=count) -> df_main_by_answerType 

df_main_by_answerType %>%
  mutate(answerType = factor(answerType, levels = c('competitor', 'sameCategory1', 'sameCategory2', 'otherCategory1', 'otherCategory2'))) %>%
  ggplot(aes(x = answerType, fill = answerType, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width = 0.2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Mean ratings") +
  xlab("Alternative type")

#ggsave("viz/priorElicitation_byAnswerOption_avg_80.pdf", width = 10, height = 6)
```

The plot below shows the raw by-vignette by-alternative ratings (small points) with labels representing the actual alternative options. The large points indicate the by-vignette by-alternative means. **Please note the varying order of the answer alternative categories on the x-axis (color).**
```{r, fig.height=36, fig.width=12, echo=FALSE}
df_clean_main_wItems_long_summary <- df_clean_main_wItems_long %>%
  group_by(answerOption_string, answerType, itemQuestion) %>% 
  summarize(mean_response = mean(centered_response)) 

df_clean_main_wItems_long2 <- left_join(df_clean_main_wItems_long, df_clean_main_wItems_long_summary, by=c('answerOption_string', 'answerType', 'itemQuestion') )

df_clean_main_wItems_long2  %>%
  mutate(answerOption_string = tidytext::reorder_within(answerOption_string, response, itemQuestion)) %>%
  ggplot(aes(x = reorder(answerOption_string, response), color = answerType, y = centered_response)) +
  geom_point(size=2, alpha=0.5) +
  geom_point(aes(y = mean_response), size=3.5) +
  geom_hline(yintercept=0, linetype='dashed') +
  tidytext::scale_x_reordered() +
  facet_wrap( itemQuestion ~ . , scales='free', ncol = 4) +
  theme(axis.text.x = element_text(angle = 55, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Rating") +
  ylim(-50, 50) +
  xlab("Alternative") 

#ggsave("viz/priorElicitation_byAnswerOption_wLabels_wMeans_woExclusions_80.pdf", width = 12, height = 36)

```


## Comparing prior ratings to free production

```{r, echo=FALSE, warning=FALSE, message=FALSE}
d_clean_main_collapsedCompetitor_summary_100 <- read_csv("d_clean_main_collapsedCompetitor_summary_100.csv")
df_clean_main_wItems_long2 <- df_clean_main_wItems_long2 %>%
  group_by(itemName, answerType) %>%
  mutate(uncentered_mean = mean(response)) %>% ungroup()
```

```{r, fig.width=13, fig.height=49, echo=FALSE, eval=FALSE}
#Below, the prior ratings are aligned with the free production data. In this plot, the item free production responses and respective prior ratings can be seen side by side.

p1 <- d_clean_main_collapsedCompetitor_summary_100 %>% 
  mutate(answerType = factor(answerType, levels=c('competitor', 'sameCategory', 'otherCategory', 'fullList', 'other', 'taciturn'))) %>%
  ggplot(aes(x = answerType, 
             fill = answerType, 
             y = responseCategory_proportion)) +
  geom_col() +
  facet_wrap( itemName ~ . , ncol = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Proportion of answer types collapsing competitors") +
  ylim(0, 1) +
  xlab("Answer type free production")

p2 <- df_clean_main_wItems_long2  %>%
  ggplot(aes(x = answerType, color = answerType, y = response)) +
  geom_point(size=2, alpha=0.5) +
  geom_point(aes(y = uncentered_mean), size=3.5) +
  facet_wrap( itemName ~ ., ncol = 1) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(2, "lines")) +
  ylab("Rating") +
  ylim(0, 100) +
  xlab("Alternative prior elicitation") 

p3 <- grid.arrange(p1, p2, ncol=2)
#ggsave("viz/freeProduction_100_vs_priorElicitation_80.pdf", p3, width=13, height=49)
```

The plot below combines free production response rates with prior ratings. More specifically, the x axis shows the categorized free production response proportions (over participants) as bars. The prior elicitation raw responses were collapsed into the categories 'competitor', 'sameCategory' (comprising ratings for 'sameCategory1' and 'sameCategory2' alternatives, respectively) and 'otherCategory' (collapsing 'otherCategory1' and 'otherCategory2' ratings). The raw responses (samller points) as well as by-item by-alternative means (larger points) are added in the respective answer categories for easier comparison. The horizontal dashed line represents no change in participants' beliefs in the prior rating experiment.

```{r, fig.width=10, fig.height=49, echo=FALSE}
df_clean_main_wItems_long2_scaled <- df_clean_main_wItems_long2 %>%
  mutate(response = response/100,
         mean_response = uncentered_mean / 100,
         answerType = case_when(
           answerType_string == "otherCategory1" ~ "otherCategory",
           answerType_string == "otherCategory2" ~ "otherCategory",
           answerType_string == "sameCategory1" ~ "sameCategory",
           answerType_string == "sameCategory2" ~ "sameCategory",
           TRUE ~ answerType_string
         ),
         answerType = factor(answerType, levels = c('competitor', 'sameCategory', 'otherCategory', 'fullList', 'other', 'taciturn')))

p4 <- d_clean_main_collapsedCompetitor_summary_100 %>% 
  mutate(answerType = factor(answerType, levels=c('competitor', 'sameCategory', 'otherCategory', 'fullList', 'other', 'taciturn'))) %>%
  ggplot(aes(x = answerType, 
             fill = answerType, 
             y = responseCategory_proportion)) +
  geom_col(alpha=0.75) +
  geom_point(data=df_clean_main_wItems_long2_scaled, aes(x=answerType, y = response, color = answerType), size=2, alpha=0.5) +
  geom_point(data=df_clean_main_wItems_long2_scaled, aes(x=answerType, y = mean_response, color = answerType), size=3.5) +
  geom_hline(yintercept = 0.5, linetype='dashed') +
  facet_wrap( itemName ~ . , ncol = 2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  theme(legend.position="none") +
  ylab("Proportion of answer types") +
  ylim(0, 1) +
  xlab("Answer type free production and prior ratings")

p4
#ggsave("viz/freeProduction_100_and_priorElicitation_80.pdf", p4, width=10, height=35)
```

## Exploratory analysis without participant exclusions

Since a relatively large proportion of participants was excluded due to attention check failure, the plot below explores whether there are qualitative differences  between the cleaned results with 60 subjects and non-cleaned results with 80 subjects. This does not seem to be the case.

```{r, fig.width=10, fig.height=49, echo=FALSE}
df_clean_main_wItems_woExclusion_long2 <- left_join(df_clean_main_wItems_woExclusions_long, df_clean_main_wItems_long_summary, by=c('answerOption_string', 'answerType', 'itemQuestion') )

df_clean_main_wItems_woExclusion_long2_scaled <- df_clean_main_wItems_woExclusion_long2 %>%
  mutate(response = response/100,
         mean_response = (mean_response + 50) / 100,
         answerType = case_when(
           answerType_string == "otherCategory1" ~ "otherCategory",
           answerType_string == "otherCategory2" ~ "otherCategory",
           answerType_string == "sameCategory1" ~ "sameCategory",
           answerType_string == "sameCategory2" ~ "sameCategory",
           TRUE ~ answerType_string
         ),
         answerType = factor(answerType, levels = c('competitor', 'sameCategory', 'otherCategory', 'fullList', 'other', 'taciturn')))

p5 <- d_clean_main_collapsedCompetitor_summary_100 %>% 
  mutate(answerType = factor(answerType, levels=c('competitor', 'sameCategory', 'otherCategory', 'fullList', 'other', 'taciturn'))) %>%
  ggplot(aes(x = answerType, 
             fill = answerType, 
             y = responseCategory_proportion)) +
  geom_col(alpha=0.75) +
  geom_point(data=df_clean_main_wItems_woExclusion_long2_scaled, aes(x=answerType, y = response, color = answerType), size=2, alpha=0.5) +
  geom_point(data=df_clean_main_wItems_woExclusion_long2_scaled, aes(x=answerType, y = mean_response, color = answerType), size=3.5) +
  geom_hline(yintercept = 0.5, linetype='dashed') +
  facet_wrap( itemName ~ . , ncol = 2) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(strip.text.x = element_text(size = 10)) +
  theme(panel.spacing = unit(3, "lines")) +
  theme(legend.position="none") +
  ylab("Proportion of answer types") +
  ylim(0, 1) +
  xlab("Answer type free production and prior ratings")

p5

```

## Extracting weakest items 

Below, the vignettes are sorted in terms of difference between mean sizes. The items with smaller differences are taken to be weaker items. 
```{r}
df_weak_items <- df_clean_main_summary_unique %>% group_by(itemName) %>%
  mutate(response_range = max(mean) - min(mean)) %>%
  arrange(response_range)

df_weak_items %>% select(itemName, response_range) %>% unique()

cat("Top 10 weakes items (worst to best): ", df_weak_items %>% pull(itemName) %>% unique() %>% .[1:10])
```

Below, several further computations try to extract in a principled way which items did not work well based on free production + slider rating results. 

For checking if the prior elicitation values are predictive of the proportion of free production response patterns (specifically, taciturn rate), several metrics on prior elicitation results can be computed:

Option 1: compute the difference between the mean (by-vignette) competitor rating and the highest alternative rating. A difference of 0 indicates that the competitor was rated highest, while a negative difference means that another option was rated higher.

Option 2: compute the difference between the competitor rating and the mean over the same category alternatives (a) or other category alternatives (b). 

Option 3: compute the average change in beliefs (i.e., the absolute difference to the rating 50 ) of the competitor (an approximation of the presence of an obvious alternative). Vignettes with smallest difference are assumed to be weakest.

For checking if vignette worked well overall, we focus on the pattern of the free production data. More specifically, we assume that the prototype response pattern would put 0.7 of responses into the competitor category and 0.3 of responses into sameCategory responses, and 0 in other response categories.

Option 4: compute a distance between the prototype distribution and the observed distribution -- Wasserstein distance

Option 5: TBD: compute mean differences on prior elicitation data but add some uncertainty term

```{r}
# option 1: compute differences between competitor and the most salient other option
df_clean_main_summary_unique_wide <- df_clean_main_summary_unique %>%
  group_by(itemName) %>%
  mutate(max_rating = max(mean)) %>% ungroup() %>%
  pivot_wider(id_cols = c("itemName", "answerType", "max_rating"), names_from = "answerType", values_from = "mean") %>%
  rowwise() %>%
  mutate(competitor_vs_maxAlternative = competitor - max_rating,
         competitor_vs_sameCategory = competitor - mean(sameCategory1, sameCategory2), # option 2a
         competitor_vs_otherCategory = competitor - mean(otherCategory1, otherCategory2), # option 2b
          sameCat_changeBeliefs = competitor-50) # option 3 
```

```{r, echo=FALSE}
# compare the worst items returned by these three options
cat("worst items according to option 1: ", df_clean_main_summary_unique_wide %>% arrange(competitor_vs_maxAlternative) %>% pull(itemName) %>% .[1:10])

cat("\nworst items according to option 2a (difference to same category): ", df_clean_main_summary_unique_wide %>% arrange(competitor_vs_sameCategory) %>% pull(itemName) %>% .[1:10])

cat("\nworst items according to option 2b (difference to other category): ", df_clean_main_summary_unique_wide %>% arrange(competitor_vs_otherCategory) %>% pull(itemName) %>% .[1:10])

cat("\nworst items according to option 3 (change in beliefs for competitor): ", df_clean_main_summary_unique_wide %>% arrange(sameCat_changeBeliefs) %>% pull(itemName) %>% .[1:10])
```

Option 4:
```{r}
d_clean_main_collapsedCompetitor_summary_100_wBaseline <- d_clean_main_collapsedCompetitor_summary_100 %>%
  mutate(expected_prop = case_when(
    answerType == "competitor" ~ 0.7,
    answerType == "sameCategory" ~ 0.3,
    TRUE ~ 0
  ))
d_clean_main_collapsedCompetitor_summary_100_wBaseline <- d_clean_main_collapsedCompetitor_summary_100_wBaseline %>% group_by(itemName) %>%
  mutate(wasserstein_dist = sum(abs(responseCategory_proportion - expected_prop)))

cat("items with the largest Wasserstein distance relative to expected free production distribution:  ", d_clean_main_collapsedCompetitor_summary_100_wBaseline %>% arrange(desc(wasserstein_dist)) %>% pull(itemName) %>% unique() %>% .[1:10])
```

Checking if either measure derived from the prior elicitation correlates well with the Wasserstein results or the raw taciturn proportion:
```{r, echo=FALSE}
wasserstein_dist_alpha <- d_clean_main_collapsedCompetitor_summary_100_wBaseline %>% select(itemName, wasserstein_dist) %>% unique() %>% arrange(itemName) %>% pull(wasserstein_dist)

competitor_vs_maxAlternative_alpha <- df_clean_main_summary_unique_wide %>% arrange(itemName) %>% pull(competitor_vs_maxAlternative)

competitor_vs_sameCategory_alpha <- df_clean_main_summary_unique_wide %>% arrange(itemName) %>% pull(competitor_vs_sameCategory)

competitor_vs_sameCategory_alpha <- df_clean_main_summary_unique_wide %>% arrange(itemName) %>% pull(competitor_vs_sameCategory)

competitor_vs_otherCategory_alpha <- df_clean_main_summary_unique_wide %>% arrange(itemName) %>% pull(competitor_vs_otherCategory)

competitor_vs_beliefsChange_alpha <- df_clean_main_summary_unique_wide %>% arrange(itemName) %>% pull(sameCat_changeBeliefs)

cat("correlation of by-vignette free production Wasserstein distances and prior elicitation differences between competitor and most salient alternative: ", cor(wasserstein_dist_alpha, competitor_vs_maxAlternative_alpha))
cat("\n\ncorrelation of by-vignette free production Wasserstein distances and prior elicitation differences between competitor and same category alternatives: ", cor(wasserstein_dist_alpha, competitor_vs_sameCategory_alpha))
cat("\n\ncorrelation of by-vignette free production Wasserstein distances and prior elicitation differences between competitor and other category alternatives: ", cor(wasserstein_dist_alpha, competitor_vs_otherCategory_alpha))
cat("\n\ncorrelation of by-vignette free production Wasserstein distances and prior elicitation changes in beliefs for competitor: ", cor(wasserstein_dist_alpha, competitor_vs_beliefsChange_alpha))

# relating the results to raw taciturn response proportions

taciturn_props <- d_clean_main_collapsedCompetitor_summary_100_wBaseline %>% filter(answerType == "taciturn") %>% arrange(itemName) %>% pull(responseCategory_proportion)
cat("correlation of by-vignette free production taciturn response proportions and prior elicitation differences between competitor and most salient alternative: ", cor(taciturn_props, competitor_vs_maxAlternative_alpha))
cat("\n\ncorrelation of by-vignette free production taciturn response proportions and prior elicitation differences between competitor and same category alternatives: ", cor(taciturn_props, competitor_vs_sameCategory_alpha))
cat("\n\ncorrelation of by-vignette free production taciturn response proportions and prior elicitation differences between competitor and other category alternatives: ", cor(taciturn_props, competitor_vs_otherCategory_alpha))
cat("\n\ncorrelation of by-vignette free production taciturn response proportions and prior elicitation changes in beliefs for competitor: ", cor(taciturn_props, competitor_vs_beliefsChange_alpha))
```

Combine all these results to see if the selected vignettes overlap. Additionally, manually extracted (by Polina) weak items are added.
```{r, echo=FALSE}
df_weakestItems_results <- tibble(
  saliencyDistance = df_clean_main_summary_unique_wide %>% arrange(competitor_vs_maxAlternative) %>% pull(itemName) %>% .[1:10],
  sameCategoryDistance = df_clean_main_summary_unique_wide %>% arrange(competitor_vs_sameCategory) %>% pull(itemName) %>% .[1:10],
  otherCategoryDistance = df_clean_main_summary_unique_wide %>% arrange(competitor_vs_otherCategory) %>% pull(itemName) %>% .[1:10],
  changeBeliefsCompetitor = df_clean_main_summary_unique_wide %>% arrange(sameCat_changeBeliefs) %>% pull(itemName) %>% .[1:10],
  wassersteinDistance = d_clean_main_collapsedCompetitor_summary_100_wBaseline %>% arrange(desc(wasserstein_dist)) %>% pull(itemName) %>% unique() %>% .[1:10],
  human_selection = c("bar-tea", "bookingAgency−lowClassAccommodation", "books-fantasy", "clothing-beach", "disney-princess", "electronics-console", "furniture-indoors", "kidsActivities-crafts", "kidsActivities-sports", "movie-fantasy") # polina's eyeballing with respect to salience of competitor + level of taciturn responses
) %>% arrange(wassersteinDistance)

df_weakestItems_results
```

Option 5: optional

## Preprocessing for RSA model fitting
```{r}
# TODO: z-scoring
```